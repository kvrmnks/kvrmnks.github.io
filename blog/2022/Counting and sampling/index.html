<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        
        
        
        <link rel="shortcut icon" href="../../../img/favicon.ico">
        <title>Counting and sampling notes - My Docs</title>
        <link href="../../../css/bootstrap.min.css" rel="stylesheet">
        <link href="../../../css/font-awesome.min.css" rel="stylesheet">
        <link href="../../../css/base.css" rel="stylesheet">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.5.0/styles/github.min.css">

        <script src="../../../js/jquery-1.10.2.min.js" defer></script>
        <script src="../../../js/bootstrap.min.js" defer></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.5.0/highlight.min.js"></script>
        <script>hljs.initHighlightingOnLoad();</script> 
    </head>

    <body>
        <div class="navbar fixed-top navbar-expand-lg navbar-dark bg-primary">
            <div class="container">
                <a class="navbar-brand" href="../../..">My Docs</a>
                <!-- Expander button -->
                <button type="button" class="navbar-toggler" data-toggle="collapse" data-target="#navbar-collapse">
                    <span class="navbar-toggler-icon"></span>
                </button>

                <!-- Expanded navigation -->
                <div id="navbar-collapse" class="navbar-collapse collapse">
                        <!-- Main navigation -->
                        <ul class="nav navbar-nav">
                            <li class="navitem">
                                <a href="../../.." class="nav-link">Welcome to kvrmnks's blog</a>
                            </li>
                            <li class="dropdown active">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Blog <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
  <li class="dropdown-submenu">
    <a href="#" class="dropdown-item">2020</a>
    <ul class="dropdown-menu">
            
<li>
    <a href="../../2020/23%E6%A0%91%E7%9A%84%E4%B8%80%E7%A7%8D%E5%88%A0%E9%99%A4%E6%96%B9%E6%B3%95/" class="dropdown-item">23树的一种删除方法</a>
</li>
            
<li>
    <a href="../../2020/3-3SAT%E9%97%AE%E9%A2%98NP%E5%AE%8C%E5%85%A8%E6%80%A7%E8%AF%81%E6%98%8E/" class="dropdown-item">3-3SAT问题NP完全性证明</a>
</li>
            
<li>
    <a href="../../2020/Burnside%E5%BC%95%E7%90%86%E4%B8%8EPolya%E8%AE%A1%E6%95%B0%E5%8E%9F%E7%90%86/" class="dropdown-item">Burnside引理与Polya计数原理</a>
</li>
            
<li>
    <a href="../../2020/k-median%E9%97%AE%E9%A2%98%E5%9C%A8Metric%E7%A9%BA%E9%97%B4%E4%B8%8A%E7%9A%84%E8%BF%91%E4%BC%BC%E8%A7%A3%E6%B3%95/" class="dropdown-item">k-median问题在Metric空间上的近似解法</a>
</li>
            
<li>
    <a href="../../2020/%E4%B8%80%E4%BA%9B%E5%AD%A6%E4%B9%A0%E8%BF%87%E7%A8%8B%E4%B8%AD%E7%94%A8%E5%88%B0%E7%9A%84%E7%94%B5%E5%AD%90%E4%B9%A6/" class="dropdown-item">一些学习过程中用到的电子书</a>
</li>
            
<li>
    <a href="../../2020/%E9%A1%B6%E7%82%B9%E8%A6%86%E7%9B%96%E9%97%AE%E9%A2%98%E5%BA%A6%E6%95%B0%E8%B4%AA%E5%BF%83%E7%AE%97%E6%B3%95%E7%9A%84%E4%B8%80%E4%B8%AA%E4%B8%8B%E7%95%8C/" class="dropdown-item">顶点覆盖问题度数贪心算法的一个下界</a>
</li>
    </ul>
  </li>
                                    
  <li class="dropdown-submenu">
    <a href="#" class="dropdown-item">2021</a>
    <ul class="dropdown-menu">
            
<li>
    <a href="../../2021/HelloWorld%E4%B8%AD%E7%9A%84%E9%98%BF%E5%B0%94%E5%A1%94%E6%8B%89%E6%98%AF%E5%90%A6%E6%98%AF%E5%9B%BE%E7%81%B5%E6%9C%BA/" class="dropdown-item">HelloWorld中的阿尔塔拉是否是图灵机</a>
</li>
            
<li>
    <a href="../../2021/NP-Complete%E9%97%AE%E9%A2%98%E4%B9%B1%E8%AF%81/" class="dropdown-item">NP-Complete问题乱证</a>
</li>
            
<li>
    <a href="../../2021/Quartus2%E4%B8%8Emodelsim%E8%81%94%E7%94%A8%E6%97%B6%E7%9A%84%E4%B8%80%E4%BA%9B%E5%9D%91/" class="dropdown-item">Quartus2与modelsim联用时的一些坑</a>
</li>
            
<li>
    <a href="../../2021/StrassenAlgorithm/" class="dropdown-item">StrassenAlgorithm</a>
</li>
            
<li>
    <a href="../../2021/X3C-1%E4%B8%8E%E5%B9%BF%E4%B9%89%E5%88%92%E5%88%86%E9%97%AE%E9%A2%98%E7%9A%84NP%E5%AE%8C%E5%85%A8%E6%80%A7%E8%AF%81%E6%98%8E/" class="dropdown-item">X3C-1与广义划分问题的NP完全性证明</a>
</li>
            
<li>
    <a href="../../2021/cmake%E7%9A%84%E5%9D%91/" class="dropdown-item">cmake的坑</a>
</li>
            
<li>
    <a href="../../2021/edge-decomposition/" class="dropdown-item">edge-decomposition</a>
</li>
            
<li>
    <a href="../../2021/push-relabel-algorithm/" class="dropdown-item">push-relabel_algorithm</a>
</li>
            
<li>
    <a href="../../2021/%E4%BA%8C%E7%BB%B4%E6%9C%80%E7%9F%AD%E8%B7%AFNP-complete/" class="dropdown-item">二维最短路NP-complete</a>
</li>
            
<li>
    <a href="../../2021/%E6%8E%B7%E8%89%B2%E5%AD%90%E5%8F%AF%E4%BB%A5%E8%AE%A9%E6%88%91%E5%8F%98%E8%81%AA%E6%98%8E%E5%90%97/" class="dropdown-item">掷色子可以让我变聪明吗</a>
</li>
            
<li>
    <a href="../../2021/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E6%B7%B7%E4%B9%B1%E6%9D%82%E8%AE%B0/" class="dropdown-item">操作系统混乱杂记</a>
</li>
            
<li>
    <a href="../../2021/%E6%AD%A3%E5%88%99%E8%AF%AD%E8%A8%80%E5%9C%A8%E5%8F%AF%E5%88%97%E4%B8%AA%E5%B9%B6%E5%92%8C%E4%BA%A4%E8%BF%90%E7%AE%97%E4%B8%8B%E7%9A%84%E4%B8%8D%E5%B0%81%E9%97%AD%E6%80%A7/" class="dropdown-item">正则语言在可列个并和交运算下的不封闭性</a>
</li>
    </ul>
  </li>
                                    
  <li class="dropdown-submenu">
    <a href="#" class="dropdown-item">2022</a>
    <ul class="dropdown-menu">
            
<li>
    <a href="../Advanced_Algorithm/" class="dropdown-item">Advanced Algorithm</a>
</li>
            
<li>
    <a href="../CS168Toolbox/" class="dropdown-item">CS168 The Modern Algorithmic Toolbox</a>
</li>
            
<li>
    <a href="./" class="dropdown-item active">Counting and sampling notes</a>
</li>
            
<li>
    <a href="../LinearAlgebraDownRight%E9%98%85%E8%AF%BB/" class="dropdown-item">LinearAlgebraDownRight阅读</a>
</li>
    </ul>
  </li>
                                </ul>
                            </li>
                        </ul>

                    <ul class="nav navbar-nav ml-auto">
                        <li class="nav-item">
                            <a href="#" class="nav-link" data-toggle="modal" data-target="#mkdocs_search_modal">
                                <i class="fa fa-search"></i> Search
                            </a>
                        </li>
                            <li class="nav-item">
                                <a rel="prev" href="../CS168Toolbox/" class="nav-link">
                                    <i class="fa fa-arrow-left"></i> Previous
                                </a>
                            </li>
                            <li class="nav-item">
                                <a rel="next" href="../LinearAlgebraDownRight%E9%98%85%E8%AF%BB/" class="nav-link">
                                    Next <i class="fa fa-arrow-right"></i>
                                </a>
                            </li>
                    </ul>
                </div>
            </div>
        </div>

        <div class="container">
            <div class="row">
                    <div class="col-md-3"><div class="navbar-light navbar-expand-md bs-sidebar hidden-print affix" role="complementary">
    <div class="navbar-header">
        <button type="button" class="navbar-toggler collapsed" data-toggle="collapse" data-target="#toc-collapse" title="Table of Contents">
            <span class="fa fa-angle-down"></span>
        </button>
    </div>

    
    <div id="toc-collapse" class="navbar-collapse collapse card bg-secondary">
        <ul class="nav flex-column">
            
            <li class="nav-item" data-level="2"><a href="#equivalence-of-counting-and-sampling" class="nav-link">Equivalence of Counting and Sampling</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            
            <li class="nav-item" data-level="2"><a href="#fpras-for-dnf-counting" class="nav-link">FPRAS for DNF counting</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            
            <li class="nav-item" data-level="2"><a href="#network-unreliability" class="nav-link">Network Unreliability</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            
            <li class="nav-item" data-level="2"><a href="#markov-chains" class="nav-link">Markov Chains</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            
            <li class="nav-item" data-level="2"><a href="#path-technology" class="nav-link">Path technology</a>
              <ul class="nav flex-column">
              </ul>
            </li>
        </ul>
    </div>
</div></div>
                    <div class="col-md-9" role="main">

<p>A notes for CSE 599: Counting and Sampling.</p>
<p>What is sampling: Assume there is a giant space <span class="arithmatex">\(\Omega\)</span>(usually finite). There is a function <span class="arithmatex">\(w: \Omega \rightarrow \mathbb{R}_{+}\)</span>. And the unknown partition function <span class="arithmatex">\(Z = \sum_{x \in \Omega} w(x)\)</span>. We need to generate a sample <span class="arithmatex">\(x\)</span> with probability <span class="arithmatex">\(\frac{w(s)}{Z}\)</span>. </p>
<p>Definition of <span class="arithmatex">\(\#P\)</span>: a function <span class="arithmatex">\(f \in \#P\)</span> if and only if it counts accept paths of a NTM of a problem in <span class="arithmatex">\(NP\)</span>. Or equally count the evidences. So #Circuit-SAT is #P-complete. And if #R is #P-complete, them R is in NP-Complete.</p>
<p>Counting and sampling deals with some #P problems.</p>
<p>Definition of multiplicative error: </p>
<p>real value <span class="arithmatex">\(p\)</span> and estimation <span class="arithmatex">\(\tilde{p}\)</span> with multiplicative error <span class="arithmatex">\(1 + \epsilon\)</span> means that </p>
<div class="arithmatex">\[(1 - \epsilon)p \leq \tilde{p} \leq (1 + \epsilon) p\]</div>
<p>or equally</p>
<div class="arithmatex">\[ |\tilde{p} - p| \leq \epsilon p\]</div>
<p><span class="arithmatex">\(p\)</span> with additive error <span class="arithmatex">\(\epsilon\)</span> means that </p>
<div class="arithmatex">\[ |\tilde{p} - p| \leq p\]</div>
<h2 id="equivalence-of-counting-and-sampling">Equivalence of Counting and Sampling</h2>
<p>The two basic problem is FPRAS and FPAUS.</p>
<h3 id="fpras">FPRAS</h3>
<p>Fully polynomial randomized approximation scheme.</p>
<p>Definition: Given a set <span class="arithmatex">\(\Omega\)</span> and a weight function <span class="arithmatex">\(w: \Omega \rightarrow \mathbb{R}_{+}\)</span> and a partition function <span class="arithmatex">\(Z = \sum_{x \in \Omega'} w(x)\)</span>, the FPRAS is an algorithm with given error rate <span class="arithmatex">\(\epsilon\)</span> and confidence interval <span class="arithmatex">\(\delta\)</span> that return <span class="arithmatex">\(\tilde{Z}\)</span> , s.t.</p>
<div class="arithmatex">\[\mathrm{Pr}[(1 - \epsilon)Z &lt; \tilde{Z} &lt; (1 + \epsilon)Z] &gt; 1 - \delta\]</div>
<p>The algorithm must run in <span class="arithmatex">\(Poly(n, \frac{1}{\epsilon}, \log \frac{1}{\delta})\)</span></p>
<h3 id="fpaus">FPAUS</h3>
<p>Fully polynomial almost uniform sampler</p>
<p>Before giving the formal definition, we have to define total variance.</p>
<h4 id="total-variance">Total variance</h4>
<p>Supppose <span class="arithmatex">\(\mu, \nu: \Omega \rightarrow \mathbb{R}_{+}\)</span> are two probability distribution. </p>
<p>The total variance is defined as </p>
<div class="arithmatex">\[ ||\mu - \nu||_{TV} = \frac{1}{2} \sum_{x \in \Omega} |\mu(x) - \nu(x)|\]</div>
<p>Or equally </p>
<div class="arithmatex">\[||\mu - \nu||_{TV} = \max_{X \subset \Omega}|\mu(X) - \nu(X)|\]</div>
<p>Proof:</p>
<p>For the second formula, we can see that if we pick <span class="arithmatex">\(X = \{x| \mu(x) &gt; \nu(x) \}\)</span>, RHS is maximized.</p>
<div class="arithmatex">\[
\begin{aligned}
|\mu(\Omega - X) - \nu(\Omega - X)| 
&amp;= |\nu(\Omega - X) - \mu(\Omega - X)| \\
&amp;= | (1 - \mu(X)) - (1 - \nu(X)) | \\
&amp;= |\nu(X) - \mu(X)| \\
&amp;= |\mu(X) - \nu(X)|  \\
\end{aligned}
\]</div>
<p>So </p>
<div class="arithmatex">\[\max_{X \subset \Omega}|\mu(X) - \nu(X)| = \frac{1}{2} \sum_{x \in \Omega}|\mu(x) - \nu(x)| \]</div>
<p>Definition of FPAUS: </p>
<p>fully polynomial almost uniform sampler</p>
<p>There is a finite space <span class="arithmatex">\(\Omega\)</span> and a weight functiton <span class="arithmatex">\(w: \Omega \rightarrow \mathbb{R}_{+}\)</span>, and a partition function <span class="arithmatex">\(Z = \sum_{x \in \Omega}w(x)\)</span>, <span class="arithmatex">\(\pi(x) = \frac{w(x)}{Z}\)</span>, fully polynomial almost uniform sampler is an algorithm with given error rate <span class="arithmatex">\(\delta\)</span> generate a sample from distribution <span class="arithmatex">\(\nu\)</span> s.t.</p>
<div class="arithmatex">\[||\mu - \nu||_{TV} &lt; \delta\]</div>
<p>running in <span class="arithmatex">\(Poly(n, \log \frac{1}{\delta})\)</span></p>
<p>For self-reducible problems, it can be proved that FPRAS means FPAUS.</p>
<h4 id="matching-counting">Matching counting</h4>
<p>In this example we show that for matching counting problem FPRAS &lt;=&gt; FPAUS.</p>
<p>For FPAUS =&gt; FPRAS</p>
<p>Assume we have FPAUS of matching counting problem.</p>
<p>We mainly consider this decomposisiont</p>
<p>Denote the matching set of graph <span class="arithmatex">\(G\)</span> as <span class="arithmatex">\(M(G)\)</span></p>
<div class="arithmatex">\[\frac{1}{|M(G)|} = \frac{|M(G_{1})|}{|M(G_{0})|} \frac{ |M(G_{2})| }{ |M(G_{1})| }  \cdots \frac{ |M(G_{m})| }{ |M(G_{m-1})| }    \]</div>
<p>Lemma 1</p>
<div class="arithmatex">\[\frac{ |M(G_{i})| }{ |M(G_{i-1})| } \geq \frac{1}{2}\]</div>
<p>proof: consider an element of <span class="arithmatex">\(G_{i}\)</span>, it does not contain <span class="arithmatex">\(e_{i}\)</span>. We can generate a distinctive matching by make one node of <span class="arithmatex">\(e_{i}\)</span> in other matching set. So </p>
<div class="arithmatex">\[ |M(G_{i})| \leq  |M(G_{i-1})| - |M(G_{i})| \]</div>
<p><span class="arithmatex">\(\blacksquare\)</span></p>
<p>Lemma 2</p>
<p>given a FPAUS of matching counting problem, that </p>
<div class="arithmatex">\[ ||\mu - \pi||_{TV} \leq K \]</div>
<p>then </p>
<div class="arithmatex">\[ |\mathbb{Pr}_{x \sim \mu}[x \in M(G_{i})]  - \mathbb{Pr}_{x \sim \pi}[x \in M(G_{i})] |\leq K\]</div>
<p>proof:</p>
<p>Let <span class="arithmatex">\(A = \{x | x \in M(G_{i})\}\)</span>, A is a subset. From total variance's definition, it's trivial.</p>
<p><span class="arithmatex">\(\blacksquare\)</span></p>
<p>We have to use lemma2 to calculate the real probability because FPAUS only generates a sample.</p>
<p>Theorem: Chernoff bound for <span class="arithmatex">\(X_{1}, X_{2}, \cdots, X_{n}\)</span> i.i.d. <span class="arithmatex">\(0 \leq X_{i} \leq 1\)</span>. Define <span class="arithmatex">\(\bar{X} = \frac{1}{n}\sum_{i=1}^{n}X_{i}\)</span>. Then for any <span class="arithmatex">\(\alpha &gt; 0\)</span></p>
<div class="arithmatex">\[\mathbb{Pr}[ |\bar{X} - \mathbb{E}[X]| \geq \alpha \mathbb{E}[X]] \leq 2e^{-n\alpha^2\mathbb{E}[X]/3}\]</div>
<p>Lemma 2.5</p>
<p>With <span class="arithmatex">\(O(\frac{m^{2}}{\epsilon^{2}}\log \frac{m}{\delta})\)</span> sample, we can approximate <span class="arithmatex">\(\bar{X}\)</span> with additive error <span class="arithmatex">\(\frac{\epsilon}{20m}\)</span> and confidence interval <span class="arithmatex">\(\frac{\delta}{m}\)</span>.</p>
<p>Use Lemma 2 and Lemma 2.5 we can approximate <span class="arithmatex">\(p_{i}\)</span> with additive error <span class="arithmatex">\(\frac{\epsilon}{10m}\)</span> and confidence interval <span class="arithmatex">\(\frac{\delta}{m}\)</span></p>
<p>Lemma 3 </p>
<p>If we approximate <span class="arithmatex">\(p\)</span>  in <span class="arithmatex">\(1 + \frac{\epsilon}{4m}\)</span> with error probability <span class="arithmatex">\(\delta\)</span>, then we can approximate <span class="arithmatex">\(\frac{1}{p}\)</span> in <span class="arithmatex">\(1 + \frac{\epsilon}{2m}\)</span></p>
<p>proof:</p>
<div class="arithmatex">\[
\mathbb{Pr}[ |\tilde{p_{i}} - p_{i} | &gt; \frac{\epsilon}{4m}p_{i}] = \mathbb{Pr}[  (1 - \frac{\epsilon}{4m})p_{i} &lt; \tilde{p}_{i} &lt; (1 + \frac{\epsilon}{4m}) p_{i} ]
\]</div>
<div class="arithmatex">\[\tilde{p}_{i} &lt; (1 + \frac{\epsilon}{4m})p_{i} \rightarrow (1 - \frac{\epsilon}{4m + \epsilon})\frac{1}{p_{i}} &lt; \frac{1}{\tilde{p}_{i}}\]</div>
<div class="arithmatex">\[(1 - \frac{\epsilon}{4m + \epsilon}) &gt; (1 - \frac{\epsilon}{2m}) \]</div>
<p>similar for the other side.</p>
<p>Lemma 4</p>
<p>If we approximate <span class="arithmatex">\(p_{i}\)</span> with additive error <span class="arithmatex">\(\epsilon\)</span>, then we can approximate it with multiplicative error <span class="arithmatex">\(1 + 2 \epsilon\)</span></p>
<p>proof:</p>
<div class="arithmatex">\[ |\tilde{p}_{i} - p_{i}| &lt; \epsilon = \epsilon 2 \frac{1}{2}  &lt; 2\epsilon p_{i} \]</div>
<p><span class="arithmatex">\(\blacksquare\)</span></p>
<p>With lemma 2, lemma 2.5 and lemma 4, we can approximate <span class="arithmatex">\(\frac{1}{p_{i}}\)</span> with multiplicative error <span class="arithmatex">\(1 + \frac{\epsilon}{5m}\)</span> and confidence interval <span class="arithmatex">\(\frac{\delta}{m}\)</span></p>
<p>Use union bound, we can prove there is an FPRAS. </p>
<p>FPRAS =&gt; FPAUS</p>
<p>Suppose we have a exact counter, we can derive a exact sampler by conditional probability method.</p>
<p>Now try to extend to approximate area.</p>
<p>Denote the <span class="arithmatex">\(i\)</span> th graph as <span class="arithmatex">\(G^{i}\)</span>, we can formulate the probability of a sample <span class="arithmatex">\(x\)</span>, <span class="arithmatex">\(e_{i} = (u_{i}, v_{i})\)</span>.</p>
<div class="arithmatex">\[\mathbb{Pr}[x] = \prod_{i}^{|E|}\max(\mathbb{I}[e_{i} \notin V(G_{i-1})], \frac{\mathbb{I}[e_{i} \notin x] |M(G^{i-1} / \{e^{i}\})| + \mathbb{I}[e_{i} \in x] |M(G^{i-1}/ \{u_{i}, v_{i}\})|}{|M(G^{i-1})|})\]</div>
<p>Using FPRAS, we can approximate <span class="arithmatex">\(|M(.)|\)</span> with multiplicative error <span class="arithmatex">\(1 + \epsilon\)</span>, and confidence interval <span class="arithmatex">\(\delta\)</span>. </p>
<p>So, we can approximate <span class="arithmatex">\(\frac{1}{\mathbb{Pr}[x]}\)</span> with multiplicative error <span class="arithmatex">\((1 + \epsilon)^{n^{2}}\)</span> and confidence interval <span class="arithmatex">\(n^{2}\delta\)</span>(union bound).</p>
<p><strong>I think process can end at here, but the lecture brings out rejection sampling that I do not understand.</strong></p>
<div class="arithmatex">\[ \tilde{\pi}(M) \geq \frac{(1 - \epsilon)^{n^2}}{|M(G)|} \geq \frac{(1 - \epsilon)^{n^2 + 1}}{|\tilde{M}(G)|} := \alpha\]</div>
<p>Once construct <span class="arithmatex">\(M\)</span>, we construct </p>
<p><span class="arithmatex">\(p_{accept}(M) = \frac{\alpha}{\tilde{\pi}(M)}\)</span></p>
<p>So every samples has the same probability. </p>
<h2 id="fpras-for-dnf-counting">FPRAS for DNF counting</h2>
<p>Assume there are <span class="arithmatex">\(n\)</span> variables and <span class="arithmatex">\(m\)</span> clauses.</p>
<p>Consider the most straight way, sample <span class="arithmatex">\(N\)</span> samples <span class="arithmatex">\(X_{1}, X_{2}, \cdots, X_{n}\)</span>. <span class="arithmatex">\(X_{i}\)</span> contains an assignment for each variable. If DNF satisfies, then <span class="arithmatex">\(X_{i} = 1\)</span></p>
<p>So
<span class="arithmatex">\(<span class="arithmatex">\(\frac{1}{N}\sum_{i=1}^{N}X_{i} = \frac{ANS}{2^{n}}\)</span>\)</span> </p>
<p>According to Chernoff Bound, if we want to sample <span class="arithmatex">\(ANS\)</span> with multiplicative error <span class="arithmatex">\(1 + \epsilon\)</span> and confidence interval <span class="arithmatex">\(\delta\)</span>. </p>
<p>We need to hold</p>
<div class="arithmatex">\[e^{-N\epsilon^{2}\frac{ANS}{3\times 2^{n}}} \leq \delta\]</div>
<p>So <span class="arithmatex">\(N &gt; \frac{2^{n}}{\epsilon^{2} ANS}\)</span>, but <span class="arithmatex">\(N\)</span> may be exp about <span class="arithmatex">\(n\)</span>.</p>
<h3 id="karp-luby-and-madras-algorithm-klm">Karp, Luby and Madras algorithm [KLM]</h3>
<p><strong>The motivation is that choose a small and count-efficient universe, then a estimator with good(polynomial) lower bound in order to use Chernoff Bound.</strong></p>
<p>This is a simple way to decrease the size of universe. </p>
<p>We sample from the union of ground set instead of all the assignments.</p>
<p>Let the ground set(feasible solutions) of <span class="arithmatex">\(i\)</span> th clause be <span class="arithmatex">\(S_{i}\)</span>.</p>
<p>We sample from <span class="arithmatex">\(\sum_{i=1}^{m} S_{i}\)</span>.</p>
<p>The estimator is <span class="arithmatex">\(\frac{|\bigcup_{i=1}^{m}S_{i}|}{\sum_{i=1}^{m} |S_{i}|}\)</span>. </p>
<p>Obviously,</p>
<div class="arithmatex">\[\frac{1}{m} \leq \frac{|\bigcup_{i=1}^{m}S_{i}|}{\sum_{i=1}^{m}S_{i}}\leq 1\]</div>
<p>This 'large' lower bound means that we can use Chernoff bound more effectively.</p>
<p>recall that <span class="arithmatex">\(\mathbb{Pr}[|X - \mathbb{E}[x]| &gt; \alpha \mathbb{E}[x]] &lt; e^{-\frac{N\alpha^{2}\mathbb{E}[x]}{3}}\)</span>. To get FPRAS, we only need</p>
<div class="arithmatex">\[e^{-\frac{N\epsilon^2 \mathbb{E}[x]}{3}} \leq e^{-\frac{N\epsilon^{2}}{3m}}\leq \delta\]</div>
<div class="arithmatex">\[N &gt;  3m\frac{1}{\epsilon^{2}}\log\frac{1}{\delta}\]</div>
<p>which achieves FPRAS</p>
<p>In DNF counting problem, it is easy to sample <span class="arithmatex">\(\sum_{i=1}^{m}|S_{i}|\)</span>, we can use brute hash to exclude those same elements.</p>
<h2 id="network-unreliability">Network Unreliability</h2>
<p>Definition: Given a graph <span class="arithmatex">\(G\)</span> with a probability function states that a link <span class="arithmatex">\(e\)</span> disappears independently with probability <span class="arithmatex">\(p_{e}\)</span>.</p>
<p>Here we assume that <span class="arithmatex">\(\forall e\in E\)</span>, <span class="arithmatex">\(p_{e} = p\)</span>. Now consider the probability that the surviving network is disconnected. Let <span class="arithmatex">\(Fail(p)\)</span> be this problem.</p>
<p>Network unreliability problem can be trivially expressed as a DNF problem.</p>
<p>Let <span class="arithmatex">\(C_{1}, C_{2}, \cdots, C_{k}\)</span> be <span class="arithmatex">\(k\)</span> cuts of graph <span class="arithmatex">\(G\)</span>.</p>
<p><span class="arithmatex">\(x_{e}\)</span> is the indicator variable that <span class="arithmatex">\(e\)</span> is broken.</p>
<p>Then <span class="arithmatex">\(G\)</span> is disconnected iff</p>
<div class="arithmatex">\[\lor _{i=1}^{k} \land_{e\in C_{i}}x_{e}\]</div>
<p>Is it solved by the KLM algorithm?</p>
<p>But it does not fit the standards of FPRAS because maybe <span class="arithmatex">\(k = \Theta(e^{n})\)</span></p>
<h3 id="kargers-algorithm">Karger's algorithm</h3>
<p><strong>Motivation: divide the problem into two parts, one is small, the other is large enough. Like the tricks in calculus.</strong></p>
<p>Let <span class="arithmatex">\(C\)</span> be the size of the minimum cut of <span class="arithmatex">\(G\)</span>. </p>
<div class="arithmatex">\[Fail(p) \geq p^{C} = q\]</div>
<p>Case 1:  <span class="arithmatex">\(q &gt; \frac{1}{poly(n)}\)</span>. We can use the Chernoff Bound directly because <strong>this property actually gives a lower bound</strong>.</p>
<p>$$$$</p>
<p>Case 2: <span class="arithmatex">\(q &lt; \frac{1}{poly(n)}\)</span>.</p>
<p>Theorem Karger's theorem. For any graph <span class="arithmatex">\(G\)</span> with <span class="arithmatex">\(n\)</span> vertices and with minimum cut <span class="arithmatex">\(C\)</span>, and for any <span class="arithmatex">\(\alpha \geq 1\)</span>, the number of cuts of size at most <span class="arithmatex">\(\alpha C\)</span> in is at most <span class="arithmatex">\(n^{2\alpha}\)</span>.</p>
<p>proof: Karger's contraction algorithm </p>
<p>Lemma: Let <span class="arithmatex">\(C_{1}, C_{2}, \cdots, C_{r}\)</span> be all the cuts of <span class="arithmatex">\(G\)</span> and let us sort them in the order of their size</p>
<div class="arithmatex">\[|C_{1}| \leq |C_{2}| \leq \cdots \leq |C_{r}|\]</div>
<p>For any <span class="arithmatex">\(\alpha \geq 1\)</span>, and <span class="arithmatex">\(q = n^{-\beta}\)</span> we have</p>
<div class="arithmatex">\[\mathbb{Pr}[\exists i \geq n^{2\alpha}: C_{i} fails] \leq \frac{n^{2\alpha(-\frac{\beta}{2}+1)}}{\frac{\beta}{2}-1}\]</div>
<p>proof:</p>
<p>recall that the theorem above actually gives the relation between the index and the cut size.</p>
<p>Let <span class="arithmatex">\(i = n^{2\alpha}\)</span>, <span class="arithmatex">\(\alpha = \frac{\log i}{2 \log n}\)</span>. So <span class="arithmatex">\(|C_{i}| \geq \frac{\log i}{2 \log n}C\)</span></p>
<p>By union bound </p>
<div class="arithmatex">\[
\begin{aligned}
\mathbb{Pr}[\exists i \geq n^{2\alpha}: C_{i} fails] &amp;\leq \sum_{i\geq n^{2\alpha}} \mathbb{Pr}[C_{i} \quad fails] \\
&amp;= \sum_{i\geq n^{2\alpha}} p^{|C_{i}|} \\
&amp;\leq \sum_{i \geq n^{2\alpha}} p^{\frac{\log i}{2\log n}C} \\
&amp;= \sum_{i \geq n^{2\alpha}}q^{\frac{\log i}{2\log n}} \\
&amp;\leq \int_{n^{2\alpha}}^{\infty}q^{\frac{\log i}{2\log n}} \mathrm{d}i \\
&amp;= \frac{n^{2\alpha(-\frac{\beta}{2} + 1)}}{\frac{\beta}{2} - 1}
\end{aligned}
\]</div>
<p>With this lemma, the "large cuts" has few probability to fail.</p>
<h2 id="markov-chains">Markov Chains</h2>
<p>Markov chains is a stochastic process on the state <span class="arithmatex">\(\Omega\)</span>. </p>
<p>Markov property </p>
<div class="arithmatex">\[\mathbb{Pr}[X_{t+1} | X_{0}, \cdots, X_{t}]  = \mathbb{Pr}[X_{t+1} | X_{t}]\]</div>
<p>Markov kenel </p>
<div class="arithmatex">\[\mathbb{Pr}[X_{t+1}|X_{t}] = K(X_{t}, X_{t+1})\]</div>
<p>Also Markov chain can be represent as a weighted direct graph, if <span class="arithmatex">\(K(x, y) = c &gt; 0\)</span> then there is a edge weighted <span class="arithmatex">\(c\)</span> from <span class="arithmatex">\(x\)</span> to <span class="arithmatex">\(y\)</span>.</p>
<p>If we sample <span class="arithmatex">\(X_{0} \sim p\)</span>. </p>
<div class="arithmatex">\[\mathbb{Pr}[X_{1}=x] = \sum_{y \in \Omega}p(y)K(y, x) = p^{T}K(x)\]</div>
<p>Corollery </p>
<div class="arithmatex">\[\mathbb{Pr}[X_{t} = y| X_{0} \sim p] = p^{T}K^{t}\]</div>
<h3 id="stationary-distribution">Stationary Distribution</h3>
<p>Every Markov chain has a stationery distribution(may not unique).</p>
<p>First of all the Markov kernel must have eigenvalue <span class="arithmatex">\(1\)</span> and corresponding eigenvector <span class="arithmatex">\(v\)</span>. </p>
<p>It is easy to show that <span class="arithmatex">\(\mathrm{det}(K - I) = 0\)</span>, as the sum of every column of <span class="arithmatex">\(K\)</span> is exactly <span class="arithmatex">\(1\)</span>.</p>
<p>Now let <span class="arithmatex">\(\pi = (|v_{1}|, |v_{2}|, \cdots, |v_{k}|)^{T}\)</span>.  We show that <span class="arithmatex">\(\pi\)</span> is a stationery distribution.</p>
<p>If <span class="arithmatex">\(\pi^{T}\pi \neq 1\)</span>, then we can normalize it into normal vector.</p>
<div class="arithmatex">\[\pi_{i} = |v_{i}| = |\sum_{j}v_{j}K(j,i)| \leq \sum_{j}|v_{j}|K(j,i) \leq \sum_{j}\pi_{j}K(j,i)\]</div>
<p>This is a trivial inequality. But the next one is somehow magic ...</p>
<div class="arithmatex">\[
\begin{aligned}
\sum_{i}\pi_{i} &amp;= \sum_{i}\pi_{i}(\sum_{j}K(i,j)) \\\
&amp;= \sum_{j}\sum_{i}\pi_{i}K(i,j) \\
&amp;\geq \sum_{j} \pi_{j}
\end{aligned}
\]</div>
<p>But actually the last inequality must be tight, so the it must holds that <span class="arithmatex">\(\pi_{i} = \sum_{j}\pi_{j}K(j,i)\)</span>.</p>
<p><strong>NB: in lecture notes it chose relu(v), but here I modified a little bit.</strong></p>
<p>Definition (Reversible Markov Chains): A Markov chain is reversible iff <strong>there exists a nonnegative weight function</strong> <span class="arithmatex">\(w: \Omega \rightarrow \mathbb{R}_{+}\)</span>. Such that for every <span class="arithmatex">\(x, y\)</span>.</p>
<div class="arithmatex">\[\pi(x)K(x, y) = \pi(y)K(y, x)\]</div>
<p>It follows that <span class="arithmatex">\(\frac{\pi}{Z}\)</span> is the stationery distribution. </p>
<h3 id="mixing-of-markov-chain">Mixing of Markov Chain</h3>
<p>Definition(Irreducible Markov Chain): A Markov chain is irreducible iff for any states <span class="arithmatex">\(x\)</span>, <span class="arithmatex">\(y\)</span>, it exits <span class="arithmatex">\(t\)</span> such that <span class="arithmatex">\(K^{t}(x,y) &gt; 0\)</span>.</p>
<p>Definition(aperiodic): A Markov Chain is aperiodic if for all <span class="arithmatex">\(x, y\)</span> we have <span class="arithmatex">\(\gcd\{t|K^{t}(x,y)&gt;0\} = 1\)</span></p>
<p>Lemma: Let <span class="arithmatex">\(K\)</span> be an irreducible, aperiodic Markov Chain. Then there exists <span class="arithmatex">\(t&gt;0\)</span> such that for all <span class="arithmatex">\(x, y\)</span></p>
<div class="arithmatex">\[K^{t}(x,y) &gt; 0\]</div>
<p><strong>Actually I do not know how to formally prove this...</strong></p>
<p>Theorem (Fundamental Theorem of Markov Chains): Any irreducible and aperiodic Markov chain has a <strong>unique</strong> stationery distribution. Futhermore, for all <span class="arithmatex">\(x, y\)</span>,</p>
<div class="arithmatex">\[K^{t}(x, y) \rightarrow \pi(y)\]</div>
<p>as t goes infinity. In particular, for any <span class="arithmatex">\(\epsilon &gt; 0\)</span> there exits <span class="arithmatex">\(t &gt; 0\)</span> such that <span class="arithmatex">\(||K^{t}(x, .) - \pi||_{TV} &lt; \epsilon\)</span>.</p>
<p>How to make an arbitrary Markov chain ergodic: 
All add a self-loop with <span class="arithmatex">\(\frac{1}{2}\)</span></p>
<h3 id="metropolis-rule">Metropolis Rule</h3>
<p>Given a finite state set and a weight function <span class="arithmatex">\(w: \Omega \rightarrow \mathbb{R}_{+}\)</span>. We would like to sample from the distribution <span class="arithmatex">\(\pi(x) = \frac{w(x)}{Z}\)</span>. </p>
<p>Metropolis rule is a general tool to construct such a ergodic Markov chain.</p>
<h4 id="neighborhood-structure">Neighborhood Structure</h4>
<p>The first requirement is a undirected connected graph <span class="arithmatex">\(G = (\Omega, E)\)</span>, two state are connected iff they different by some local changes. </p>
<h4 id="proposal-distribution">Proposal Distribution</h4>
<p>At any vertex <span class="arithmatex">\(x\)</span> we require a proposal distribution, <span class="arithmatex">\(p(x, .)\)</span> satisfying the following properties:</p>
<ol>
<li><span class="arithmatex">\(p(x,y) &gt; 0\)</span> only if <span class="arithmatex">\(y\)</span> is a neighbor of <span class="arithmatex">\(x\)</span>.</li>
<li><span class="arithmatex">\(p(x,y) = p(y,x)\)</span> for all <span class="arithmatex">\(y\)</span></li>
<li><span class="arithmatex">\(\sum_{y}p(x,y) = 1\)</span></li>
</ol>
<h4 id="metropolis-chain">Metropolis chain</h4>
<ol>
<li>decide a propose move from <span class="arithmatex">\(x\)</span> to <span class="arithmatex">\(y\)</span> with probability <span class="arithmatex">\(p(x, y)\)</span>.</li>
<li>accept the propose move with probability <span class="arithmatex">\(\min\{1, \frac{\pi(y)}{\pi(x)}\}\)</span>. else reject and stay at <span class="arithmatex">\(x\)</span></li>
</ol>
<p><strong>NB: this is like the simulated annealing ideas in optimization</strong></p>
<p>Lemma: Metropolis chain is reversible with stationery distribution <span class="arithmatex">\(\pi\)</span></p>
<p>(But how to prove that <span class="arithmatex">\(\pi\)</span> is the stationery distribution?)</p>
<h3 id="coupling">Coupling</h3>
<p>Definition(Coupling): Let <span class="arithmatex">\(\mu, \nu\)</span> be probability distributions over <span class="arithmatex">\(\Omega\)</span>, A coupling between <span class="arithmatex">\(\mu, \nu\)</span> is a probability distribution <span class="arithmatex">\(\pi\)</span> on <span class="arithmatex">\(\Omega \times \Omega\)</span> that preserves the marginals of <span class="arithmatex">\(\mu, \nu\)</span>. </p>
<div class="arithmatex">\[\sum_{y} \pi(x, y) = \mu(x)\]</div>
<div class="arithmatex">\[\sum_{x} \pi(x, y) = \nu(y)\]</div>
<p>Lemma(Coupling Lemma): Let <span class="arithmatex">\(X \sim \mu\)</span>, <span class="arithmatex">\(Y \sim \nu\)</span></p>
<p>Then </p>
<ol>
<li><span class="arithmatex">\(\mathbb{Pr}[X \neq Y] \geq ||\mu - \nu||_{TV}\)</span></li>
<li>There exists a coupling <span class="arithmatex">\(\pi\)</span> between <span class="arithmatex">\(\mu\)</span> and <span class="arithmatex">\(\nu\)</span> such that <span class="arithmatex">\(\mathbb{Pr}[X \neq Y] = ||\mu - \nu||_{TV}\)</span></li>
</ol>
<p>proof:</p>
<p>A simple observation is that <span class="arithmatex">\(\mathbb{Pr}[X=Y=a] \leq \min\{\mu(a), \nu(a)\}\)</span></p>
<div class="arithmatex">\[
\begin{aligned}
\mathbb{Pr}[X \neq Y] &amp;= 1 - \sum_{a \in \Omega} \mathbb{Pr}[X = Y = a]\\
&amp;\leq 1 - \sum_{a \in \Omega} \min\{\mu(a), \nu(a)\} \\
&amp;= \sum_{a \in \Omega} (\mu(a) - \min\{\mu(a), \nu(a)\}) \\
&amp;= ||\mu - \nu||_{TV}
\end{aligned}
\]</div>
<p>So we just need to "properly" set  remaining probability to let the inequality be tight.</p>
<p><span class="arithmatex">\(\blacksquare\)</span></p>
<h4 id="mixing-time">Mixing time</h4>
<p>Terminology:</p>
<div class="arithmatex">\[\Delta_{x}^{t} = ||K(x,.)^{t} - \pi||_{TV}\]</div>
<div class="arithmatex">\[\tau_{x}(\epsilon) = \min\{t|\Delta_{x}^{t} &lt; \epsilon\}\]</div>
<div class="arithmatex">\[\tau (\epsilon) = \max \{\tau_{x}(\epsilon)| x\in \Omega \}\]</div>
<p>Definition (Mixing time): <span class="arithmatex">\(\tau(\frac{1}{2e})\)</span></p>
<p>Lemma: </p>
<div class="arithmatex">\[\Delta_{x}(t+1) \leq \Delta_{x}(t)\]</div>
<p>proof:</p>
<p>This proof shows the power of coupling.</p>
<p>Let <span class="arithmatex">\(X_{0} = x\)</span>, <span class="arithmatex">\(Y_{0} \sim \pi\)</span>.</p>
<p>Suppose now we get a coupling such that <span class="arithmatex">\(\mathbb{Pr}[X_{t}\neq Y_{t}] = \Delta_{x}(t)\)</span></p>
<p>Now define <span class="arithmatex">\(X_{t+1}\)</span>, <span class="arithmatex">\(Y_{t+1}\)</span> as below</p>
<ol>
<li>
<p>If <span class="arithmatex">\(X_{t} = Y_{t}\)</span>, then set <span class="arithmatex">\(X_{t+1} = Y_{t+1}\)</span></p>
</li>
<li>
<p>else random walk independently</p>
</li>
</ol>
<p>So 
<span class="arithmatex">\(<span class="arithmatex">\(\Delta_{x}(t+1) \leq \mathbb{Pr}[X_{t+1} \neq Y_{t+1}] \leq \mathbb{Pr}[X_{t}\neq Y_{t}] = \Delta_{x}(t)\)</span>\)</span></p>
<p><span class="arithmatex">\(\blacksquare\)</span></p>
<p>General bound for mixing time</p>
<p>Lemma:</p>
<div class="arithmatex">\[\tau_{mix} \leq \frac{1}{|\Omega|\min_{x,y} K(x,y)^{2}}\]</div>
<p>proof:</p>
<p>Pick the same coupling</p>
<div class="arithmatex">\[
\begin{aligned}
\Delta_{x}(t+1) &amp;\leq \mathbb{Pr}[X_{t+1} \neq Y_{t+1}] \\
&amp;= \mathbb{Pr}[X_{t+1}\neq Y_{t+1}|X_{t} \neq Y_{t}]\Delta_{x}(t)\\
&amp;= (1 - \sum_{x} K(X_{t}, x) K(Y_{t}, x))\Delta_{x}(t)\\
&amp;\leq (1 - |\Omega|\min_{x, y} K(x, y)^2)\Delta_{x}(t)
\end{aligned}
\]</div>
<p>So</p>
<div class="arithmatex">\[\Delta_{x}(t) \leq (1 - |\Omega|\min_{x, y} K(x, y)^2)^{t}\]</div>
<p>Let 
<span class="arithmatex">\(<span class="arithmatex">\((1 - |\Omega|\min_{x, y} K(x, y)^2)^{t} \leq \frac{1}{2e}\)</span>\)</span></p>
<p><span class="arithmatex">\(\blacksquare\)</span></p>
<p>The above also proves the Fundamental Theorem of Markov Chain.</p>
<p>Theorem: For any Markov Chain, <span class="arithmatex">\(\tau(\epsilon) \leq O(\tau_{mix}\log\frac{1}{\epsilon})\)</span></p>
<h4 id="coloring">Coloring</h4>
<p>Try to assign <span class="arithmatex">\(q\)</span> colors for a graph with maximum degree <span class="arithmatex">\(\Delta\)</span>.</p>
<p>We want to sample proper color assignments.</p>
<p>Define the coupling as follows <span class="arithmatex">\((X, Y)\)</span>.</p>
<p>Randomly choose a vertex and a color, try to change the color in both <span class="arithmatex">\(X\)</span> and <span class="arithmatex">\(Y\)</span> of corresponding vertex.</p>
<div class="arithmatex">\[
\begin{aligned}
\mathbb{E}[d(X_{t+1}, Y_{t+1})] &amp;= \mathbb{Pr}[A](d(X_{t}, Y_{t})) + \mathbb{Pr}[B](d(X_{t}, Y_{t}) + 1) + \mathbb{Pr}[C](d(X_{t}, Y_{t}) - 1)\\
&amp;\leq d(X_{t}, Y_{t}) + \mathbb{Pr}[B] - \mathbb{Pr}[C] \\
&amp;\leq d(X_{t}, Y_{t}) + \frac{2\Delta d(X_{t}, Y_{t})}{nq} - \frac{d(X_{t}, Y_{t})(q - 2\Delta)}{nq}\\
&amp;= (1 - \frac{q - 4\Delta}{nq})d(X_{t}, Y_{t})
\end{aligned}
\]</div>
<p>Also <span class="arithmatex">\(d(X_{0}, Y_{0}) \leq n\)</span></p>
<p>So</p>
<div class="arithmatex">\[\mathbb{E}[d(X_{t}, Y_{t})] \leq n(1 - \frac{q-4\Delta}{nq})^{t}\]</div>
<div class="arithmatex">\[\mathbb{Pr}[d(X_{t}, Y_{t}) \geq 1] \leq \mathbb{E}[d(X_{t}, Y_{t})] \leq n(1 - \frac{q-4\Delta}{nq})^{t} \leq \delta\]</div>
<p>So to achieve <span class="arithmatex">\(\frac{1}{n}\)</span> error, we need <span class="arithmatex">\(O(nq\log n)\)</span> steps. </p>
<p><strong>Also we need <span class="arithmatex">\(q \geq 4\Delta + 1\)</span></strong></p>
<h3 id="path-coupling">Path coupling</h3>
<p>Path coupling is defined on the pre-metric. And it is the property that holds for adjacent vertices and can be extend to the whole <span class="arithmatex">\(\Omega\)</span>.</p>
<p>Definition (pre-metric): A pre-metric defined on <span class="arithmatex">\(\Omega\)</span> holds that for any adjacent edge <span class="arithmatex">\(uv\)</span>, <span class="arithmatex">\(d(uv) = d(u, v)\)</span>.</p>
<p>Definition (path coupling): Define <span class="arithmatex">\((X', Y')\)</span> is a coupling define on a pre-metric graph. If for any adjacent vertices <span class="arithmatex">\((x, y)\)</span> holds that </p>
<div class="arithmatex">\[\mathbb{E}[d(X', Y')|(x, y)] \leq (1 - \alpha)d(x, y)\]</div>
<p>Then for every pair vertices, they all hold the above inequality.</p>
<p>insight: The original coupling is about to converge. And the uniform weighted graph is naturally holds the pre-metric.</p>
<p>proof:</p>
<p>For any pair of vertices <span class="arithmatex">\((s, t)\)</span>. </p>
<p>Chose an arbitrary shortest path <span class="arithmatex">\((s=u_{0}, u_{1}, u_{2}, \cdots, u_{k}, t=u_{k+1})\)</span>.</p>
<p>Naturally extend the coupling into multi-vertices coupling <span class="arithmatex">\((U_{0}, U_{1}, U_{2}, \cdots, U_{k}, U_{k+1})\)</span>.</p>
<div class="arithmatex">\[
\begin{aligned}
\mathbb{E}[d(S, T)] &amp;= \mathbb{E}[\sum_{i=0}^{k}d(U_{i}, U_{i+1})] \\
&amp;= \sum_{i=0}^{k}\mathbb{E}[d(U_{i}, U_{i+1})] \\
&amp;\leq \sum_{i=0}^{k}(1-\alpha)d(u_{i}, u_{i+1})\\
&amp;= (1-\alpha)d(s, t)
\end{aligned}
\]</div>
<p><span class="arithmatex">\(\blacksquare\)</span></p>
<h4 id="coloring-with-path-coupling-theorem">Coloring with Path coupling theorem</h4>
<p>Theorem: If <span class="arithmatex">\(q \geq 2\Delta + 1\)</span>, then the metropolis rule mixes in time <span class="arithmatex">\(O(n\log n)\)</span>.</p>
<p>First we need to modify the graph to let it be a pre-metric graph in order to use path coupling theorem. </p>
<p>Consider the metric <span class="arithmatex">\(d(X, Y)\)</span> calculating how many different colors of color configuration <span class="arithmatex">\(X\)</span> and <span class="arithmatex">\(Y\)</span>. Sometime we cannot find a path of proper exchange, so we need to allow improper color configuration. Such that there are total <span class="arithmatex">\(q^{n}\)</span> vertices in the graph.</p>
<p>We can use metropolis rules to let their probability is <span class="arithmatex">\(0\)</span> in <span class="arithmatex">\(\pi\)</span>. Also if we bound the total variance, we can still bound the "real" total variance. </p>
<p>Then we need to design the path coupling procedure. We can only consider <span class="arithmatex">\(d(X', Y'|x, y)\)</span>, <span class="arithmatex">\(x\)</span> and <span class="arithmatex">\(y\)</span> are adjacent with path coupling.</p>
<p>Assume the exact different color of <span class="arithmatex">\(x\)</span> and <span class="arithmatex">\(y\)</span> are on the vertex <span class="arithmatex">\(u\)</span>.</p>
<p>In general, we choose a vertex <span class="arithmatex">\(v\)</span> and a color <span class="arithmatex">\(c\)</span> randomly, then try to change <span class="arithmatex">\(v\)</span>'s color into <span class="arithmatex">\(c\)</span>.</p>
<p>This is not the complete procedure, but we can try to analyze this. We denote <span class="arithmatex">\(N(u) \bigcup \{u\} = N^{*}(u)\)</span>. </p>
<p>If <span class="arithmatex">\(v\)</span> is not in <span class="arithmatex">\(N^{*}(u)\)</span>, then <span class="arithmatex">\(d(X', Y') = d(x, y)\)</span>. </p>
<p>If <span class="arithmatex">\(v = u\)</span>, then <span class="arithmatex">\(\mathbb{E}[d(X', Y')] \leq \frac{\Delta}{q}d(x, y) + (1 - \frac{\Delta}{q})(d(x, y) - 1)\)</span>.</p>
<p>If <span class="arithmatex">\(v \in N^{*}(u)\)</span>, but <span class="arithmatex">\(u \neq v\)</span>, if <span class="arithmatex">\(c = c_{x}\)</span> or <span class="arithmatex">\(c = c_{y}\)</span> then <span class="arithmatex">\(d(X, Y) \leq d(x, y) + 1\)</span> otherwise <span class="arithmatex">\(d(X, Y) = d(x, y)\)</span>.</p>
<p>Sum all the inequalities above, we get</p>
<div class="arithmatex">\[\mathbb{E}[(X', Y')|(x, y)] \leq d(x, y) - \frac{q - 3\Delta}{nq} = 1 - \frac{q - 3\Delta}{nq} \leq (1 - \frac{1}{nq})d(x, y)\]</div>
<p>When <span class="arithmatex">\(q \geq 3\Delta + 1\)</span>.</p>
<p>To improve this bound, we need to specify when <span class="arithmatex">\(c = c_{x}\)</span> or <span class="arithmatex">\(c = c_{y}\)</span>, we can reorder the configuration. To achieve</p>
<div class="arithmatex">\[\mathbb{E}[(X', Y')|(x, y)] \leq d(x, y) - \frac{q - 3\Delta}{nq} = 1 - \frac{q - 3\Delta}{nq} \leq (1 - \frac{1}{nq})d(x, y)\]</div>
<p>Which we only need <span class="arithmatex">\(q \geq 2\Delta + 1\)</span>.</p>
<h3 id="coloring-with-heat-bath-chain">Coloring with Heat bath chain</h3>
<p>The main motivation of the path of metropolis rule is that try to construct a coupling such that the probability of <span class="arithmatex">\(d(x, y) + 1\)</span> is low.</p>
<p>One trivial way is to increase <span class="arithmatex">\(q\)</span>. Beyond that path coupling consider just consider vertices pairs that are adjacent in order to decrease the "controversy".</p>
<p>Now let's focus on heat bath chain.</p>
<p>Instead of metropolis rule that randomly pick a vertex and a color then change the configuration. Heat bath chain randomly chooses a vertex then samples from the proper color.</p>
<p>(Obviously it may break the pre-metric requirement)</p>
<p>Here is a simple way: randomly chooses a vertex <span class="arithmatex">\(u\)</span>. Denote the proper colors for <span class="arithmatex">\(u\)</span> under configuration <span class="arithmatex">\(X\)</span> as <span class="arithmatex">\(A(X, u)\)</span>. </p>
<p>Then for configuration <span class="arithmatex">\(X, Y\)</span>, we sample from <span class="arithmatex">\(\max(|A(X, u)|, |A(Y, u)|)\)</span>. With probability <span class="arithmatex">\(\frac{|A(X, u) \bigcap A(Y, u)|}{\max(|A(X, u)|, |A(Y, u)|)}\)</span> we choose the same color. Obviously we can properly assign other events.</p>
<p>Then we analyze this coupling</p>
<div class="arithmatex">\[\mathbb{E}[d(X', Y')|(x, y)] = \sum_{v}\mathbb{E}[c_{X'}(v) \neq c_{Y'}(v)]\]</div>
<div class="arithmatex">\[\mathbb{E}[c_{X'}(v) \neq c_{Y'}(v)] = \mathbb{Pr}[u=v]\mathbb{Pr}[c_{X'}(v) \neq c_{Y'}(v)|u=v] + \mathbb{Pr}[u\neq v]\mathbb{Pr}[c_{X'}(v) \neq c_{Y'}(v)|u\neq v]\]</div>
<div class="arithmatex">\[
\begin{aligned}
\mathbb{Pr}[c_{X'}(v)\neq c_{Y'}(v)|u=v] &amp;= 1 - \frac{|A(X, u) \bigcap A(Y, u)|}{\max(|A(X, u)|, |A(Y, u)|)} \\
&amp;\leq \frac{1}{q - \Delta}|u\sim v: c_{x}(v) \neq c_{y}(u)|
\end{aligned}
\]</div>
<div class="arithmatex">\[
\mathbb{Pr}[c_{X'}(v) \neq c_{Y'}(v)|u\neq v] = \frac{d(x, y)}{n}
\]</div>
<p>Sum all these equalities and inequalities</p>
<div class="arithmatex">\[
\mathbb{E}[d(X', Y')|(x, y)] \leq \frac{\Delta d(x, y)}{n(q - \Delta)} + (1 - \frac{1}{n})d(x, y)
\]</div>
<p>If <span class="arithmatex">\(q \geq 2\Delta + 1\)</span>,</p>
<div class="arithmatex">\[
\mathbb{E}[d(X', Y')|(x, y)] \leq (1 - \frac{1}{n(\Delta + 1)})d(x, y)
\]</div>
<p>So for heat-bath, <span class="arithmatex">\(q \geq 2\Delta + 1\)</span> and trivial coupling method lead to <span class="arithmatex">\(\Theta(n \log n)\)</span> mixing time.</p>
<h4 id="triangle-free-graph">Triangle-free graph</h4>
<p>Now image that the given graph is triangle-free. Then <span class="arithmatex">\(A(X, u)\)</span> can be very large. Maybe better than <span class="arithmatex">\(q - \Delta\)</span> which can leads to a better result.</p>
<p>Now analyze the <span class="arithmatex">\(A(X, u)\)</span></p>
<div class="arithmatex">\[A(X, u) = \sum_{c} \prod_{v \sim u} (1 - X_{v, c})\]</div>
<p>For triangle-free graph <span class="arithmatex">\(\mathbb{E}[\prod_{v \sim u} (1 - X_{v, c})] = \prod_{v \sim u} (1 - \mathbb{E}[X_{v, c}])\)</span></p>
<p>We denote the information about <span class="arithmatex">\(V - N^{*}(u)\)</span> as <span class="arithmatex">\(\mathcal{F}\)</span>.</p>
<div class="arithmatex">\[
\begin{aligned}
\mathbb{E}[A(X, u)|\mathcal{F}] &amp;= \sum_{c}\prod_{v\sim u}(1 - \mathbb{E}[X_{v, c}]) \\
&amp;= \sum_{c}\prod_{v\sim u}(1 - \frac{1}{|A(X, v)|}) \\
&amp;\leq  q\prod_{c}\prod_{v\sim u}(1 - \frac{1}{|A(X, v)|})^{\frac{1}{q}} \\
&amp;= q \prod_{v\sim u}\prod_{c\in A(X, v)} (1 - \frac{1}{|A(X, v)|})^{\frac{1}{q}} \\
&amp;\leq q e^{-\frac{\Delta}{q}}
\end{aligned}
\]</div>
<p>Recall the McDiarmid's inequality. </p>
<p>Let <span class="arithmatex">\(X_{v,c}\)</span> s be the variables (NB: here are actually <span class="arithmatex">\(d(u)\)</span> variables), <span class="arithmatex">\(f(.) = |A(X, u)|\)</span>.  </p>
<p><span class="arithmatex">\(f\)</span> is 1-Lipschitz function.</p>
<p>So </p>
<div class="arithmatex">\[\mathbb{Pr}[|f(x_{1}, \cdots, x_{n}) - \mathbb{E}f(x_{1}, \cdots, x_{n})| \geq t ] \leq 2e^{-\frac{t^{2}}{2\Delta}}\]</div>
<p>So </p>
<div class="arithmatex">\[\mathbb{Pr}[|A(X, u)| \leq q e^{-\frac{\Delta}{q}}(1 - \epsilon)] \leq 2e^{-\epsilon^{2}q}\]</div>
<p>By union bound,</p>
<div class="arithmatex">\[\mathbb{Pr}[\exists u:|A(X, u)| \leq q e^{-\frac{\Delta}{q}}(1 - \epsilon)] \leq 2ne^{-\epsilon^{2}q}\]</div>
<p><span class="arithmatex">\(\blacksquare\)</span></p>
<h3 id="mixing-time-using-eigenvalues">Mixing time using eigenvalues</h3>
<p>Here we consider <strong>reversible Markov Chains</strong>.</p>
<p>For any function <span class="arithmatex">\(f, g\)</span>, we consider inner-product space as</p>
<div class="arithmatex">\[\langle f, g \rangle = \sum_{x} f(x)g(x)\pi(x)\]</div>
<div class="arithmatex">\[||f|| = \langle f, f \rangle = \sum_{x}f^{2}(x)\pi(x)\]</div>
<p>It follows that <span class="arithmatex">\(K\)</span> is self-adjoint.</p>
<p>With spectral theorem, <span class="arithmatex">\(K\)</span> has <span class="arithmatex">\(n\)</span> eigenvalues.</p>
<p>By stochasticity, <span class="arithmatex">\(\forall i \leq n, |\lambda_{i}| \leq 1\)</span></p>
<p>Some results about the characteristics of reversible Markov Chain.</p>
<p>irreducible &lt;-&gt; <span class="arithmatex">\(\lambda_{2} &lt; 1\)</span></p>
<p>aperiodic &lt;-&gt; <span class="arithmatex">\(\lambda_{|\Omega|} &gt; -1\)</span></p>
<p>Why eigenvalues are "<span class="arithmatex">\(l_{2}\)</span> properties" about the chain?</p>
<p>Definition: (<span class="arithmatex">\(l_p\)</span>- distance) </p>
<div class="arithmatex">\[ \Vert  \frac{K^t(x, \cdot)}{\pi(\cdot)} - 1 \Vert_p = \left (\sum_{y} \pi(y) |\frac{K^{t}(x, y)}{\pi(y)} - 1|^p \right )^{\frac{1}{p}}\]</div>
<p>Compare with <span class="arithmatex">\(|| \cdot ||_{TV}\)</span></p>
<p><span class="arithmatex">\(\cdot||_{TV} = \frac{1}{2} ||\cdot||_1 \leq \frac{1}{2}||\cdot||_2\)</span></p>
<p>Is this a general rule?</p>
<p>Theorem: Let <span class="arithmatex">\(\lambda^{*} = \max\{\lambda_{2}, |\lambda_n|\}\)</span>, </p>
<div class="arithmatex">\[\Vert \frac{K^{t}(x, \cdot)}{\pi(\cdot)}  - 1\Vert_{2} \leq \frac{\lambda^{*^{2t}}}{\pi(x)}\]</div>
<p>Consider</p>
<div class="arithmatex">\[\Vert \frac{K^{t}(x, \cdot)}{\pi(\cdot)}  - 1\Vert_{2} = \langle \frac{K^{t}(x, \cdot)}{\pi(\cdot)}  - 1, \frac{K^{t}(x, \cdot)}{\pi(\cdot)}  - 1\rangle\]</div>
<p>Then project <span class="arithmatex">\(\frac{K^{t}(x, \cdot)}{\pi(\cdot)}  - 1\)</span> into <span class="arithmatex">\(K\)</span> space. </p>
<p>Notice: <span class="arithmatex">\(\boldsymbol{1} = \psi_1\)</span></p>
<div class="arithmatex">\[ \frac{K^t(x, \cdot)}{\pi(\cdot)} - 1 = \sum_{i &gt; 1} \langle \frac{K^t(x, \cdot)}{\pi(\cdot)}, \psi_i  \rangle  \psi_i  = \sum_{i &gt; 1}\lambda_i^t \psi_i(x)\psi_i\]</div>
<div class="arithmatex">\[\Vert \frac{K^t(x, \cdot)}{\pi(\cdot)} - 1  \Vert_2 = \sum_{i &gt; 1}\lambda_i^{2t} \psi_i^2(x) \leq \lambda^{*} \sum_{i&gt;1} \psi_i^2(x) \leq \frac{\lambda^{*^{2t}}}{\pi(x)}\]</div>
<p>Such a magic</p>
<div class="arithmatex">\[
\begin{aligned}
\sum_{i} \psi_i^2(x) &amp;= \sum_{i} \langle \psi_i, \frac{1_x}{\pi} \rangle ^2 \\
&amp;= 
\Vert \sum_i \langle \psi_i, \frac{1_x}{\pi} \rangle \psi_i \Vert ^2 \\
&amp;= 
\Vert \frac{1_x}{\pi} \Vert ^2 \\
&amp;= \frac{1}{\pi(x)}
\end{aligned} 
\]</div>
<p>select a vertex with high probability as warm start.</p>
<h2 id="path-technology">Path technology</h2>
<h3 id="dirichlet-form">Dirichlet Form</h3>
<p>For two functions <span class="arithmatex">\(f, g\)</span>, define </p>
<div class="arithmatex">\[\mathcal{E}(f,g) = \left \langle (I - K)f, g \right \rangle\]</div>
<p>(easy to see that <span class="arithmatex">\(I - K\)</span> is self-adjoint)</p>
<p><span class="arithmatex">\(\mathcal{E}(f, g) = \frac{1}{2}\sum_{x, y} (f(x) - f(y))(g(x) - g(y))\pi(x)K(x, y)\)</span></p>
<p>NB: actually we just need to enumerate all the edge. The reason is that if <span class="arithmatex">\((x, y) \notin E\)</span>, then <span class="arithmatex">\(K(x, y) = 0\)</span></p>
<p>Dirichlet form of <span class="arithmatex">\(f\)</span></p>
<p><span class="arithmatex">\(\mathcal{E}(f, f) = \frac{1}{2}\sum_{x, y} (f(x) - f(y))^2 \pi(x) K(x, y)\)</span></p>
<p>Variance of <span class="arithmatex">\(f\)</span></p>
<div class="arithmatex">\[Var(f) = \Vert f - \mathbb{E}_{\pi}f  \Vert^2 = \frac{1}{2}\sum_{x, y}(f(x) - f(y))^2\pi(x)\pi(y)\]</div>
<p>Definition: Poincare constant </p>
<div class="arithmatex">\[\alpha = \min_{f} \frac{\mathcal{E}(f, f)}{Var(f)}\]</div>
<p>If <span class="arithmatex">\(K\)</span> is reversible, then poincare constant is <span class="arithmatex">\(1 - \lambda_2\)</span></p>
<p>For a lazy chain with Poincare constant <span class="arithmatex">\(\alpha\)</span>,</p>
<div class="arithmatex">\[\tau_x(\epsilon) \leq O(\frac{\log \frac{1}{\epsilon \pi(x)}}{\alpha})\]</div>
<p>So if we can bound the poincare constant, we can bound the mixing time.</p>
<p>Consider the multicommodity flow problem(reversible Markov chain),</p>
<p>Define </p>
<div class="arithmatex">\[f(e) = \sum_{e \in P_{x, y}}\pi(x)\pi(y)\]</div>
<div class="arithmatex">\[Q(e=uv) = \pi(u)K(u, v) = \pi(v)K(v, u)\]</div>
<p>Define the congestion of an edge <span class="arithmatex">\(e\)</span> as <span class="arithmatex">\(\frac{f(e)}{Q(e)}\)</span></p>
<p>For any reversible Markov chain, for any two states <span class="arithmatex">\(x, y \in \Omega\)</span>, </p>
<div class="arithmatex">\[\frac{1}{\alpha} \leq \max_{e} \frac{f(e)}{Q(e)} \cdot \max_{x, y}|P_{x, y}|\]</div>
<p>Actually, <span class="arithmatex">\(P_{x, y}\)</span> can be bounded by the diameter of the graph then the <span class="arithmatex">\(n\)</span>.</p>
<p>proof:
For any function <span class="arithmatex">\(f\)</span></p>
<div class="arithmatex">\[
\begin{aligned}
Var(f) 
&amp;= 
\frac{1}{2}\sum_{x, y} (f(x) - f(y))^2\pi(x)\pi(y) \\
&amp;= 
\frac{1}{2}\sum_{x, y} (\sum_{e \in P_{x,y}}(f(e^+) - f(e^-)) )^2\pi(x)\pi(y) \\
&amp;\leq \frac{1}{2}\sum_{x, y} |P_{x,y}|\sum_{e \in P_{x,y}}(f(e^+) - f(e^-))^2\pi(x)\pi(y) \\
&amp;\leq  \max_{x,y}|P_{x,y}| \cdot \frac{1}{2}\sum_{x, y} \sum_{e \in P_{x,y}}(f(e^+) - f(e^-))^2\pi(x)\pi(y) \\
&amp; \leq \max_{x,y}|P_{x,y}| \cdot \frac{1}{2} \sum_{e}(f(e^+) - f(e^-))^2 \sum_{e \in P_{x, y}} \pi(x)\pi(y) \\
&amp;= \max_{x,y}|P_{x,y}| \cdot \frac{1}{2} \sum_{e}(f(e^+) - f(e^-))^2 Q(e) \frac{f(e)}{Q(e)} \\
&amp;= \max_{x,y}|P_{x,y}| \cdot \frac{1}{2} \sum_{e=(x,y)}(f(e^+) - f(e^-))^2 \pi(x)K(x, y) \frac{f(e)}{Q(e)} \\
&amp;= \max_{x,y}|P_{x,y}| \cdot \mathcal{E}(f, f) \frac{f(e)}{Q(e)} (\text{If x, y are not connected then} K(x,y)=0) \\
\end{aligned}
\]</div>
<p>Fractional version</p>
<p>Define <span class="arithmatex">\(\mu_{x, y}(P)\)</span> as the probability of choosing <span class="arithmatex">\(P\)</span> as the path from <span class="arithmatex">\(x \rightarrow y\)</span>.</p>
<p>Then update the definition for <span class="arithmatex">\(f(e)\)</span></p>
<div class="arithmatex">\[f(e) = \sum_{x, y} \pi(x) \pi(y)\sum_{e\in P \sim \mu_{x,y}} \mu_{x, y}(P)\]</div>
<p>Now calculate the Poincare constant</p>
<div class="arithmatex">\[
\begin{aligned}
Var(f) 
&amp;= 
\frac{1}{2}\sum_{x, y} (f(x) - f(y))^2\pi(x)\pi(y) \\
&amp;= 
\frac{1}{2} \pi(x)\pi(y) \sum_{x, y} (\sum_{P}\mu_{x, y}(P)\sum_{e\in P}(f(e^+) - f(e^-)) )^2 \\
&amp;\leq \frac{1}{2}\sum_{x, y} |P_{x,y}|\sum_{e \in P_{x,y}}(f(e^+) - f(e^-))^2(\sum_{P}\mu_{x, y}^{2}(P))\pi(x)\pi(y) \\
&amp;\leq \frac{1}{2}\sum_{x, y} |P_{x,y}|\sum_{e \in P_{x,y}}(f(e^+) - f(e^-))^2\pi(x)\pi(y) \\
&amp;\leq  \max_{x,y}|P_{x,y}| \cdot \frac{1}{2}\sum_{x, y} \sum_{e \in P_{x,y}}(f(e^+) - f(e^-))^2\pi(x)\pi(y) \\
&amp; \leq \max_{x,y}|P_{x,y}| \cdot \frac{1}{2} \sum_{e}(f(e^+) - f(e^-))^2 \sum_{e \in P_{x, y}} \pi(x)\pi(y) \\
&amp;= \max_{x,y}|P_{x,y}| \cdot \frac{1}{2} \sum_{e}(f(e^+) - f(e^-))^2 Q(e) \frac{f(e)}{Q(e)} \\
&amp;= \max_{x,y}|P_{x,y}| \cdot \frac{1}{2} \sum_{e=(x,y)}(f(e^+) - f(e^-))^2 \pi(x)K(x, y) \frac{f(e)}{Q(e)} \\
&amp;= \max_{x,y}|P_{x,y}| \cdot \mathcal{E}(f, f) \frac{f(e)}{Q(e)} (\text{If x, y are not connected then} K(x,y)=0) \\
\end{aligned}
\]</div>
<p>So the same theorem holds.</p>
<p><span class="arithmatex">\(\blacksquare\)</span></p>
<p>There are some intrinsic gaps between this bound and the tight bound by <span class="arithmatex">\(\log n\)</span>.</p>
<h3 id="all-or-nothing-theorem">All or nothing theorem</h3>
<p>Def of self-reducible problem:</p>
<p>For every instance of one specific NP search problem, if the set of its solutions can be divided into polynomial size subsets corresponding to the same NP search problem with smaller instances(these instances can be generated in polynomial time). Then this NP search problem is self-reducible problem.</p>
<p>Is this def the same as Sipser?</p>
<p>Thm: For any self-reducible problem, if there exists a polynomial time counting algorithm that gives <span class="arithmatex">\(1 + poly(n)\)</span> multiplicative error, then there is an FPRAS.</p>
<p>(Recall that for self-reducible problem, <span class="arithmatex">\(\text{FPRAS}  \leftrightarrow \text{FPAUS}\)</span>)</p>
<p>The basic intuition is that we prove that there is a FPAUS.</p>
<p>If we can use the polynomial approximation factor to build a Markov chain with fast mixing time.</p>
<p>The good situation is that every vertex(state) in Markov chain represents a proper answer(element needed to be counted).</p>
<p>Of the proportion of proper vertices is large enough, like <span class="arithmatex">\(\frac{1}{poly(n)}\)</span>.</p>
<p>So we can derive the FPRAS.</p>
<p>Here We will use the DNF(different from the counting problem in the notes) as a demonstration. </p>
<p>Consider a tree from the root to the leaves, one step downward means that a variable is assigned. </p>
<p>Define a order of variables <span class="arithmatex">\(l_1, l_2, \cdots, l_n\)</span>, At level <span class="arithmatex">\(i\)</span>, assign the <span class="arithmatex">\(v_i\)</span> true of false.</p>
<p>Basically this is a decision tree, every node in the tree is a configuration.</p>
<p>Denote the actual count of configuration <span class="arithmatex">\(x\)</span> as <span class="arithmatex">\(N(x)\)</span>, the FPRAS with parameter <span class="arithmatex">\(\alpha\)</span> returns a answer <span class="arithmatex">\(\hat{N}(x)\)</span>, <span class="arithmatex">\(\frac{N(x)}{\alpha} \leq \hat{N}(x) \leq \alpha N(x)\)</span>.</p>
<p>For simplicity, we can assume <span class="arithmatex">\(\hat{N}(x)\)</span> is deterministic. (Otherwise we can run <span class="arithmatex">\(\log \frac{1}{\epsilon}\)</span> to boost).</p>
<p>For an edge <span class="arithmatex">\((e, v)\)</span>, w.l.o.g. <span class="arithmatex">\(e\)</span> is the father of <span class="arithmatex">\(v\)</span>. </p>
<p>Define <span class="arithmatex">\(w(e, v) = \hat{N}(v)\)</span>. To make it a Markov chain, for a specific vertex <span class="arithmatex">\(x\)</span>, whose father is <span class="arithmatex">\(a\)</span>, sons are <span class="arithmatex">\(b\)</span> and <span class="arithmatex">\(c\)</span>. Define <span class="arithmatex">\(Z_{x} = \hat{N}(x) + \hat{N}(b) + \hat{N}(c)\)</span>, <span class="arithmatex">\(K(x, a) = \frac{\hat{N}(x)}{Z_{x}}\)</span>, <span class="arithmatex">\(K(x, b) = \frac{\hat{N}(b)}{Z_{x}}\)</span> and <span class="arithmatex">\(K(x, c) = \frac{\hat{N}(c)}{Z_{x}}\)</span>.</p>
<p>Now consider random walk on this graph. Define <span class="arithmatex">\(Z = \sum_{x}Z_{x}\)</span>. It follows that <span class="arithmatex">\(\pi(x) = \frac{Z_{x}}{Z}\)</span>. </p>
<p>Assume <span class="arithmatex">\(x\)</span> is the father of <span class="arithmatex">\(y\)</span> in the following equation.</p>
<div class="arithmatex">\[\sum_{(x, y) \in E} \pi(x)K(x, y) = \sum_{(x, y) \in E} \frac{Z_{x}}{Z} \frac{\hat{N}(y)}{Z_{x}} = \frac{Z_{y}}{Z} = \pi(y)\]</div>
<p>Each assignment is actually a leaf. Because leaves have the same level, so they has the same probability(has only one edge connecting to its father).</p>
<p>Then we need to bound the sum of leaves' probability.</p>
<p>Lemma: random walk ends at leaves with probability <span class="arithmatex">\(\frac{O(1)}{\alpha n}\)</span></p>
<p>proof:</p>
<p>Use <span class="arithmatex">\(L\)</span> to denote the actual amount of leaves. </p>
<p>Observe that the sum of the weight of edges from level <span class="arithmatex">\(i\)</span> to level <span class="arithmatex">\(i+1\)</span> is at most <span class="arithmatex">\(\alpha L\)</span>.</p>
<p>Because there are at <span class="arithmatex">\(n\)</span> levels, so sum of all the weight is at most <span class="arithmatex">\(2\alpha Ln\)</span></p>
<p>The probability that ends at leaves is at least <span class="arithmatex">\(\frac{1}{2\alpha n}\)</span></p>
<p><span class="arithmatex">\(\blacksquare\)</span></p>
<p>That's <span class="arithmatex">\(\frac{1}{poly(n)}\)</span></p>
<p>Now we need to bound the mixing time.</p>
<p>Although the structure of tree means that there is only simple path, it seems not easy to use the path coupling lemma.</p>
<p>We choose path technology here.</p>
<p>For a specific edge <span class="arithmatex">\(e = (u, v)\)</span>, <span class="arithmatex">\(u\)</span> is the father of <span class="arithmatex">\(v\)</span>.</p>
<p><span class="arithmatex">\(f(e) = (\sum_{x \in son(v)} \pi(x)) (1 - \sum_{x \in son(v)} \pi(x)) \leq \frac{1}{Z}2\alpha n N(v)\)</span></p>
<p><span class="arithmatex">\(Q(e) = \pi(u)K(u,v) = \frac{Z_u}{Z} \frac{\hat{N}(v)}{Z_{u}} = \frac{\hat{N}(v)}{Z}\)</span></p>
<p><span class="arithmatex">\(\max_{e} \frac{f(e)}{Q(e)} \leq 2\alpha^2 n\)</span></p>
<p><span class="arithmatex">\(\max_{x, y} |P_{x, y}| \leq 2n\)</span></p>
<p>By the path technology</p>
<div class="arithmatex">\[\tau_x(\epsilon) \leq O(2\alpha^2n^2\log \frac{1}{\epsilon \pi(x)})\]</div>
<p>Choose x as root. Then <span class="arithmatex">\(\pi(root) = \frac{Z_{root}}{Z} \geq \frac{1}{2\alpha n}\)</span></p>
<p>So,</p>
<div class="arithmatex">\[\tau_x(\epsilon) \leq O(2\alpha^2n^2\log \frac{2\alpha n}{\epsilon})\]</div>
<p>So we can get a FPAUS.</p>
<h3 id="implementation">Implementation</h3>
<p>Random walk on hypercube.</p>
<p>First consider using the path coupling.</p>
<p>Obviously the metric satisfies the pre-metric constrains.</p>
<p>consider an edge <span class="arithmatex">\(e = (x, y)\)</span>.</p>
<p>coupling with randomly picking a bit then turn to the same neighbor.</p>
<div class="arithmatex">\[
\begin{aligned}
\mathbb{E}[d(X, Y|x, y)] &amp;= (1 - \frac{1}{n})d(x, y)
\end{aligned}
\]</div>
<p>So <span class="arithmatex">\(d(X, Y) \leq n(1 - \frac{1}{n})^{T}\)</span>, <span class="arithmatex">\(T = O(n\log n)\)</span></p>
<!-- $$
\begin{aligned}
\mathbb{E}[A(X, u)] &= \mathbb{E}[\sum_{c} \prod_{v \sim u} (1 - X_{v, c})] \\
&= \mathbb{E}[\sum_{c} \prod_{v \sim u} (1 - X_{v, c})]
\end{aligned}
$$ -->

<!-- Let  $W = \frac{|A(X, u) \bigcap A(Y, u)|}{\max(|A(X, u)|, |A(Y, u)|)}$

$$
\begin{aligned}
\mathbb{E}[d((X', Y')|(x, y))] &= \mathbb{Pr}[c_{X}(u) \neq c_{Y}(u)](W(d(x, y) - 1) + (1 - W)d(x, y)) + \mathbb{Pr}[c_{X}(u) = c_{Y}(u)](W(d(x, y)) + (1 - W)(1 + d(x, y)))\\
&\leq \mathbb{Pr}[c_{X}(u) \neq c_{Y}(u)][d(x, y) - 1] + 1 - W\\
&\leq \frac{d(x, y)}{n} [d(x, y) - 1] + \frac{d(x, y)}{n(q - \Delta)}

\end{aligned}
$$ -->

<!-- d(x, y) + (1 - \frac{|A(X, u) \bigcap A(Y, u)|}{\max(|A(X, u)|, |A(Y, u)|)})  --></div>
            </div>
        </div>

        <footer class="col-md-12">
            <hr>
            <p>Documentation built with <a href="https://www.mkdocs.org/">MkDocs</a>.</p>
        </footer>
        <script>
            var base_url = "../../..",
                shortcuts = {"help": 191, "next": 78, "previous": 80, "search": 83};
        </script>
        <script src="../../../js/base.js" defer></script>
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6" defer></script>
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" defer></script>
        <script src="../../../search/main.js" defer></script>

        <div class="modal" id="mkdocs_search_modal" tabindex="-1" role="dialog" aria-labelledby="searchModalLabel" aria-hidden="true">
    <div class="modal-dialog modal-lg">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="searchModalLabel">Search</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
                <p>From here you can search these documents. Enter your search terms below.</p>
                <form>
                    <div class="form-group">
                        <input type="search" class="form-control" placeholder="Search..." id="mkdocs-search-query" title="Type search term here">
                    </div>
                </form>
                <div id="mkdocs-search-results" data-no-results-text="No results found"></div>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div><div class="modal" id="mkdocs_keyboard_modal" tabindex="-1" role="dialog" aria-labelledby="keyboardModalLabel" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="keyboardModalLabel">Keyboard Shortcuts</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
              <table class="table">
                <thead>
                  <tr>
                    <th style="width: 20%;">Keys</th>
                    <th>Action</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td class="help shortcut"><kbd>?</kbd></td>
                    <td>Open this help</td>
                  </tr>
                  <tr>
                    <td class="next shortcut"><kbd>n</kbd></td>
                    <td>Next page</td>
                  </tr>
                  <tr>
                    <td class="prev shortcut"><kbd>p</kbd></td>
                    <td>Previous page</td>
                  </tr>
                  <tr>
                    <td class="search shortcut"><kbd>s</kbd></td>
                    <td>Search</td>
                  </tr>
                </tbody>
              </table>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div>

    </body>
</html>

<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        
        
        
        <link rel="shortcut icon" href="../../../img/favicon.ico">
        <title>CS168 The Modern Algorithmic Toolbox - My Docs</title>
        <link href="../../../css/bootstrap.min.css" rel="stylesheet">
        <link href="../../../css/font-awesome.min.css" rel="stylesheet">
        <link href="../../../css/base.css" rel="stylesheet">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.5.0/styles/github.min.css">

        <script src="../../../js/jquery-1.10.2.min.js" defer></script>
        <script src="../../../js/bootstrap.min.js" defer></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.5.0/highlight.min.js"></script>
        <script>hljs.initHighlightingOnLoad();</script> 
    </head>

    <body>
        <div class="navbar fixed-top navbar-expand-lg navbar-dark bg-primary">
            <div class="container">
                <a class="navbar-brand" href="../../..">My Docs</a>
                <!-- Expander button -->
                <button type="button" class="navbar-toggler" data-toggle="collapse" data-target="#navbar-collapse">
                    <span class="navbar-toggler-icon"></span>
                </button>

                <!-- Expanded navigation -->
                <div id="navbar-collapse" class="navbar-collapse collapse">
                        <!-- Main navigation -->
                        <ul class="nav navbar-nav">
                            <li class="navitem">
                                <a href="../../.." class="nav-link">Welcome to kvrmnks's blog</a>
                            </li>
                            <li class="dropdown active">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Blog <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
  <li class="dropdown-submenu">
    <a href="#" class="dropdown-item">2020</a>
    <ul class="dropdown-menu">
            
<li>
    <a href="../../2020/23%E6%A0%91%E7%9A%84%E4%B8%80%E7%A7%8D%E5%88%A0%E9%99%A4%E6%96%B9%E6%B3%95/" class="dropdown-item">23树的一种删除方法</a>
</li>
            
<li>
    <a href="../../2020/3-3SAT%E9%97%AE%E9%A2%98NP%E5%AE%8C%E5%85%A8%E6%80%A7%E8%AF%81%E6%98%8E/" class="dropdown-item">3-3SAT问题NP完全性证明</a>
</li>
            
<li>
    <a href="../../2020/Burnside%E5%BC%95%E7%90%86%E4%B8%8EPolya%E8%AE%A1%E6%95%B0%E5%8E%9F%E7%90%86/" class="dropdown-item">Burnside引理与Polya计数原理</a>
</li>
            
<li>
    <a href="../../2020/k-median%E9%97%AE%E9%A2%98%E5%9C%A8Metric%E7%A9%BA%E9%97%B4%E4%B8%8A%E7%9A%84%E8%BF%91%E4%BC%BC%E8%A7%A3%E6%B3%95/" class="dropdown-item">k-median问题在Metric空间上的近似解法</a>
</li>
            
<li>
    <a href="../../2020/%E4%B8%80%E4%BA%9B%E5%AD%A6%E4%B9%A0%E8%BF%87%E7%A8%8B%E4%B8%AD%E7%94%A8%E5%88%B0%E7%9A%84%E7%94%B5%E5%AD%90%E4%B9%A6/" class="dropdown-item">一些学习过程中用到的电子书</a>
</li>
            
<li>
    <a href="../../2020/%E9%A1%B6%E7%82%B9%E8%A6%86%E7%9B%96%E9%97%AE%E9%A2%98%E5%BA%A6%E6%95%B0%E8%B4%AA%E5%BF%83%E7%AE%97%E6%B3%95%E7%9A%84%E4%B8%80%E4%B8%AA%E4%B8%8B%E7%95%8C/" class="dropdown-item">顶点覆盖问题度数贪心算法的一个下界</a>
</li>
    </ul>
  </li>
                                    
  <li class="dropdown-submenu">
    <a href="#" class="dropdown-item">2021</a>
    <ul class="dropdown-menu">
            
<li>
    <a href="../../2021/HelloWorld%E4%B8%AD%E7%9A%84%E9%98%BF%E5%B0%94%E5%A1%94%E6%8B%89%E6%98%AF%E5%90%A6%E6%98%AF%E5%9B%BE%E7%81%B5%E6%9C%BA/" class="dropdown-item">HelloWorld中的阿尔塔拉是否是图灵机</a>
</li>
            
<li>
    <a href="../../2021/NP-Complete%E9%97%AE%E9%A2%98%E4%B9%B1%E8%AF%81/" class="dropdown-item">NP-Complete问题乱证</a>
</li>
            
<li>
    <a href="../../2021/Quartus2%E4%B8%8Emodelsim%E8%81%94%E7%94%A8%E6%97%B6%E7%9A%84%E4%B8%80%E4%BA%9B%E5%9D%91/" class="dropdown-item">Quartus2与modelsim联用时的一些坑</a>
</li>
            
<li>
    <a href="../../2021/StrassenAlgorithm/" class="dropdown-item">StrassenAlgorithm</a>
</li>
            
<li>
    <a href="../../2021/X3C-1%E4%B8%8E%E5%B9%BF%E4%B9%89%E5%88%92%E5%88%86%E9%97%AE%E9%A2%98%E7%9A%84NP%E5%AE%8C%E5%85%A8%E6%80%A7%E8%AF%81%E6%98%8E/" class="dropdown-item">X3C-1与广义划分问题的NP完全性证明</a>
</li>
            
<li>
    <a href="../../2021/cmake%E7%9A%84%E5%9D%91/" class="dropdown-item">cmake的坑</a>
</li>
            
<li>
    <a href="../../2021/edge-decomposition/" class="dropdown-item">edge-decomposition</a>
</li>
            
<li>
    <a href="../../2021/push-relabel-algorithm/" class="dropdown-item">push-relabel_algorithm</a>
</li>
            
<li>
    <a href="../../2021/%E4%BA%8C%E7%BB%B4%E6%9C%80%E7%9F%AD%E8%B7%AFNP-complete/" class="dropdown-item">二维最短路NP-complete</a>
</li>
            
<li>
    <a href="../../2021/%E6%8E%B7%E8%89%B2%E5%AD%90%E5%8F%AF%E4%BB%A5%E8%AE%A9%E6%88%91%E5%8F%98%E8%81%AA%E6%98%8E%E5%90%97/" class="dropdown-item">掷色子可以让我变聪明吗</a>
</li>
            
<li>
    <a href="../../2021/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E6%B7%B7%E4%B9%B1%E6%9D%82%E8%AE%B0/" class="dropdown-item">操作系统混乱杂记</a>
</li>
            
<li>
    <a href="../../2021/%E6%AD%A3%E5%88%99%E8%AF%AD%E8%A8%80%E5%9C%A8%E5%8F%AF%E5%88%97%E4%B8%AA%E5%B9%B6%E5%92%8C%E4%BA%A4%E8%BF%90%E7%AE%97%E4%B8%8B%E7%9A%84%E4%B8%8D%E5%B0%81%E9%97%AD%E6%80%A7/" class="dropdown-item">正则语言在可列个并和交运算下的不封闭性</a>
</li>
    </ul>
  </li>
                                    
  <li class="dropdown-submenu">
    <a href="#" class="dropdown-item">2022</a>
    <ul class="dropdown-menu">
            
<li>
    <a href="../Advanced_Algorithm/" class="dropdown-item">Advanced Algorithm</a>
</li>
            
<li>
    <a href="./" class="dropdown-item active">CS168 The Modern Algorithmic Toolbox</a>
</li>
            
<li>
    <a href="../Counting%20and%20sampling/" class="dropdown-item">Counting and sampling notes</a>
</li>
            
<li>
    <a href="../LinearAlgebraDownRight%E9%98%85%E8%AF%BB/" class="dropdown-item">LinearAlgebraDownRight阅读</a>
</li>
    </ul>
  </li>
                                    
  <li class="dropdown-submenu">
    <a href="#" class="dropdown-item">2023</a>
    <ul class="dropdown-menu">
            
<li>
    <a href="../../2023/AI%E5%AD%A6%E4%B9%A0/" class="dropdown-item">AI学习</a>
</li>
    </ul>
  </li>
                                </ul>
                            </li>
                        </ul>

                    <ul class="nav navbar-nav ml-auto">
                        <li class="nav-item">
                            <a href="#" class="nav-link" data-toggle="modal" data-target="#mkdocs_search_modal">
                                <i class="fa fa-search"></i> Search
                            </a>
                        </li>
                            <li class="nav-item">
                                <a rel="prev" href="../Advanced_Algorithm/" class="nav-link">
                                    <i class="fa fa-arrow-left"></i> Previous
                                </a>
                            </li>
                            <li class="nav-item">
                                <a rel="next" href="../Counting%20and%20sampling/" class="nav-link">
                                    Next <i class="fa fa-arrow-right"></i>
                                </a>
                            </li>
                    </ul>
                </div>
            </div>
        </div>

        <div class="container">
            <div class="row">
                    <div class="col-md-3"><div class="navbar-light navbar-expand-md bs-sidebar hidden-print affix" role="complementary">
    <div class="navbar-header">
        <button type="button" class="navbar-toggler collapsed" data-toggle="collapse" data-target="#toc-collapse" title="Table of Contents">
            <span class="fa fa-angle-down"></span>
        </button>
    </div>

    
    <div id="toc-collapse" class="navbar-collapse collapse card bg-secondary">
        <ul class="nav flex-column">
            
            <li class="nav-item" data-level="2"><a href="#dimensionality-reduction" class="nav-link">Dimensionality Reduction</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            
            <li class="nav-item" data-level="2"><a href="#learning-binary-classification-here" class="nav-link">Learning (Binary classification here)</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            
            <li class="nav-item" data-level="2"><a href="#principle-component-analysis" class="nav-link">Principle Component Analysis</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            
            <li class="nav-item" data-level="2"><a href="#low-rank-matrix-decomposition" class="nav-link">Low-rank Matrix Decomposition</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            
            <li class="nav-item" data-level="2"><a href="#low-rank-tensor-decomposition" class="nav-link">Low-rank tensor Decomposition</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            
            <li class="nav-item" data-level="2"><a href="#spectral-graph-theory" class="nav-link">Spectral Graph Theory</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            
            <li class="nav-item" data-level="2"><a href="#sampling-and-estimation" class="nav-link">Sampling and estimation</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            
            <li class="nav-item" data-level="2"><a href="#discrete-fourier-transform-and-convolution" class="nav-link">Discrete Fourier Transform and Convolution</a>
              <ul class="nav flex-column">
              </ul>
            </li>
        </ul>
    </div>
</div></div>
                    <div class="col-md-9" role="main">

<p>This is my notes about the CS168 in Stanford University</p>
<h2 id="dimensionality-reduction">Dimensionality Reduction</h2>
<p>Want to reduce the large dimension into small dimension ( usually independent with input size )  while preserving distance.</p>
<h3 id="distance">Distance</h3>
<p>distance or similarity</p>
<p><strong>exact</strong> distance: 1 - [x==y]</p>
<p>similarity of set ( or multi-set ): Jaccard Similarity <span class="arithmatex">\(J(S,T) = \frac{|S \bigcap T|}{|S\bigcup T|}\)</span></p>
<p><span class="arithmatex">\(l_{p}\)</span> distance.</p>
<h3 id="framework">framework</h3>
<p>reduce dimension while preserving expect.</p>
<p>Then use independent trials.</p>
<h3 id="exact-distance">exact distance</h3>
<p>use universal hash function assumption</p>
<p>assume <span class="arithmatex">\(h: U \rightarrow [b]+1\)</span></p>
<p>k hash function <span class="arithmatex">\(h_{1}, h_{2}, ... , h_{k}\)</span></p>
<h4 id="epsilon-heavy-hitters"><span class="arithmatex">\(\epsilon\)</span> -  Heavy Hitters</h4>
<div class="arithmatex">\[\# = \min_{i=1}^{k} B(f_{i}(x)) = \min_{i=1}^{k} \sum_{j=1}^{n}[f_{i}(x) = f_{j}(x)]\]</div>
<div class="arithmatex">\[E[\#] = \min_{i=1}^{k} \sum_{j=1}^{n}E[[f_{i}(x) = f_{j}(x)]] = \#x + \frac{n-\#x}{b} \leq \#x + \frac{n}{b}\]</div>
<p>use Markov's inequality</p>
<div class="arithmatex">\[\mathrm{Pr}[\# - \#x &gt; \delta \frac{n}{b}] \leq \frac{1}{\delta}\]</div>
<p>to achieve <span class="arithmatex">\((\delta, \epsilon)\)</span>, let <span class="arithmatex">\(b = \frac{2}{\epsilon}\)</span>, <span class="arithmatex">\(\delta = 2\)</span>.</p>
<p>Then use independent trials <span class="arithmatex">\(\frac{1}{2^{p}} &lt; \delta\)</span>, then <span class="arithmatex">\(p = \log(\frac{1}{\delta})\)</span>.</p>
<h3 id="jaccard-similarity">Jaccard similarity</h3>
<p>pick a random permutation of <span class="arithmatex">\(U\)</span>. </p>
<p>hash each element to the smallest element.</p>
<h3 id="l_2-distance-johnson-lindenstrauss-transform"><span class="arithmatex">\(l_{2}\)</span> distance - Johnson-Lindenstrauss transform</h3>
<p>motivation: random projection (inner product with a vector generated by standard Gaussian distribution)</p>
<p>random vector  <span class="arithmatex">\(\textbf{r} \sim N^{n}(0, 1)\)</span></p>
<div class="arithmatex">\[X = \textbf{x}_{1}\textbf{r} - \textbf{x}_{2}\textbf{r} = \sum_{i=1}^{n} (x_{1,i} - x_{2, i})r_{i} \sim N(0, l_{2}^2)\]</div>
<p>which means that <span class="arithmatex">\(X\)</span> is an unbiased estimator of <span class="arithmatex">\(l^{2}_{2}\)</span>.</p>
<h3 id="other-distances">other distances ...</h3>
<p>what about cosine similarity, edit distance and wasserstein distance.</p>
<h2 id="learning-binary-classification-here">Learning (Binary classification here)</h2>
<p>Assume data are samples from a prior distribution <span class="arithmatex">\(D\)</span> on the universe <span class="arithmatex">\(U\)</span>.</p>
<p>element has its only label.</p>
<p>mainly cares about the sample complexity.</p>
<h3 id="finite-well-separated-case">finite well-separated case</h3>
<p>finite: there is a function set <span class="arithmatex">\(\{f_{1}, ..., f_{h}\}\)</span> including the ground truth <span class="arithmatex">\(f^{*}\)</span>.</p>
<p>well-separated: if <span class="arithmatex">\(f_{i} \neq f^{*}\)</span>, then the generalization error is at least <span class="arithmatex">\(\epsilon\)</span>.</p>
<p>Theorem: if</p>
<div class="arithmatex">\[n \geq \frac{1}{\epsilon}(\ln h+\ln \frac{1}{\delta} )\]</div>
<p>Then with probability <span class="arithmatex">\(1 - \delta\)</span>, the output <span class="arithmatex">\(f = f^{*}\)</span>.</p>
<p>PROOF:</p>
<p><span class="arithmatex">\(Pr[ f=^{X}f^{*} | f\neq f^{*}] \leq (1-\epsilon)^{n} \leq e^{-\epsilon n}\)</span></p>
<p><span class="arithmatex">\(Pr[\exists f_{i} = f^{*}] \leq (h-1)e^{-\epsilon n} \leq he^{-\epsilon n}\)</span></p>
<p><span class="arithmatex">\(he^{-\epsilon n} \leq \delta\)</span></p>
<p><span class="arithmatex">\(n \geq \frac{1}{\epsilon}(\ln h + \ln \frac{1}{\delta})\)</span></p>
<p>Q.E.D</p>
<h3 id="finite-case">finite case</h3>
<p>Theorem: if</p>
<div class="arithmatex">\[n \geq \frac{c}{\epsilon}(\ln h+\ln \frac{1}{\delta})\]</div>
<p>Then with probability <span class="arithmatex">\(1 - \delta\)</span>, the output <span class="arithmatex">\(f\)</span>'s generalization error is less than <span class="arithmatex">\(\epsilon\)</span>.</p>
<p>easy to drive from previous theorem.</p>
<h3 id="linear-classifiers">Linear Classifiers</h3>
<p>Throw the assumption of finite function set.</p>
<p>However there is still the ground true that has the form of linear classifier.</p>
<p>Theorem: if</p>
<div class="arithmatex">\[n \geq \frac{c}{\epsilon}(d + \ln \frac{1}{\delta})\]</div>
<p>Then there is a constant <span class="arithmatex">\(c\)</span>, with probability <span class="arithmatex">\(1 - \delta\)</span>, the generalization error is less than <span class="arithmatex">\(\epsilon\)</span>.</p>
<p>proof motivation: the curse of dimensionality (approximation).</p>
<p>note the number of samples is linear to the dimension.</p>
<p>(because we use about <span class="arithmatex">\(e^{d}\)</span> functions to approximate in theory, but we usually calculate the superplane to express the function)</p>
<h3 id="non-zero-training-error-and-the-erm-algorithm">Non-Zero Training Error and the ERM Algorithm</h3>
<p>ERM (empirical risk minimization)</p>
<p>just output the function with minimum training error.</p>
<p>Theorem (Uniform Convergence):</p>
<p>if</p>
<div class="arithmatex">\[n \geq \frac{c}{\epsilon ^2}(d + \ln \frac{1}{\delta})\]</div>
<p>then for every linear classifier, it holds that </p>
<p>generalization error in training error +- <span class="arithmatex">\(\epsilon\)</span></p>
<p>Theorem (PAC for non-zero training error):</p>
<p>easy to drive from previous theorem</p>
<h3 id="increasing-dimensionality">Increasing Dimensionality</h3>
<p>according to the previous section, when samples are larger than (or linear to) dimension, it will lead to best-fit.</p>
<p>but for non-zero error case, we do not know the dimension exactly. </p>
<p>So we may need to flirt with line between overfitting and generalization.</p>
<p>when n &lt;&lt; d, we need to increase the dimension.</p>
<h4 id="polynomial-embedding">Polynomial embedding</h4>
<p>just add cross-product into the higher dimension.</p>
<p>It is better when the dimension is meaningful.</p>
<h4 id="jl-transform">JL transform</h4>
<p>actually we need to apply some non-linear operator to each dimension.</p>
<p>Because JL transform actually the combination of each dimension.</p>
<p>In real world, the kernel function is a good way to implement.</p>
<h3 id="regularization">Regularization</h3>
<p>Regularization states that you have some preference of your model.</p>
<p>There are usually two views about the effect of regularization.</p>
<p>Bayesian view and frequentist view</p>
<h4 id="bayesian-view">Bayesian view</h4>
<p>Here the regularization comes naturally from the likelihood.</p>
<p>For example, we assume the model is <span class="arithmatex">\(y = \left \langle x, a\right \rangle + z\)</span></p>
<p><span class="arithmatex">\(z \sim N(0, 1)\)</span>, <span class="arithmatex">\(a_{i} \sim N(0, \sigma^2)\)</span></p>
<p>we assume that <span class="arithmatex">\(x\)</span>'s are fixed for simplicity.</p>
<p>The likelihood is </p>
<div class="arithmatex">\[\mathrm{Pr}(a)\mathrm{Pr}(\frac{data}{a}) = \prod_{i}^{d}e^{-\frac{a^2_{i}}{2\sigma^2}}\prod_{i}^{n}e^{-\frac{(y_{i}-\left \langle x_{i}, a \right \rangle)^{2}}{2}}\]</div>
<p>max this likelihood means minimize <span class="arithmatex">\(\sum_{i=1}^{d}\frac{a_{i}^{2}}{2\sigma^2} + \sum_{i=1}^{n} (y_{i} - \left \langle x_{i}, a\right \rangle)^2\)</span></p>
<p>The first part is regularization.</p>
<p>Also it shows that the regularization may depend on the hypothesis of model.</p>
<h4 id="frequentist-view">Frequentist view</h4>
<p>The given example is about <span class="arithmatex">\(l_{0}\)</span> regularization. (define <span class="arithmatex">\(0^0 = 0\)</span>)</p>
<p><span class="arithmatex">\(l_{0}\)</span> regularization shows the sparsity. </p>
<h5 id="l_1-regularization"><span class="arithmatex">\(l_{1}\)</span> regularization</h5>
<p>it can be a proxy of <span class="arithmatex">\(l_{0}\)</span> regularization.</p>
<h2 id="principle-component-analysis">Principle Component Analysis</h2>
<p>The motivation is that we want to map the data into a <span class="arithmatex">\(d\)</span> - dimension vector space.</p>
<p>Somehow we want to preserve the <span class="arithmatex">\(|\prod_{S}v| \sim |v|\)</span></p>
<p>Luckily, we know that <span class="arithmatex">\(d\)</span> - dimension space can be interpret into a span of d vectors <span class="arithmatex">\(v_{1}, ..., v_{d}\)</span>.</p>
<p>And the objective function is <span class="arithmatex">\(\max \sum_{i=1}^{n} \sqrt{\sum_{j=1}^{d} \left \langle x_{i}, v_{j} \right \rangle ^{2}}\)</span>.</p>
<p>why max? Because of the triangle inequality.</p>
<p>Usually we compact the data as a matrix <span class="arithmatex">\(A\)</span> whose columns states for attributes and row stand for pieces of data.</p>
<p>We create a new matrix <span class="arithmatex">\(X\)</span> of vectors <span class="arithmatex">\(v\)</span> whose <span class="arithmatex">\(i\)</span> th column is <span class="arithmatex">\(v_{i}\)</span>, in order to model the objective function as matrix operation.</p>
<p>The objective function is <span class="arithmatex">\((AX)^{T}AX = X^{T}A^{T}AX = X^{T}U^{T}DUX\)</span> according to spectrum theorem.</p>
<p><span class="arithmatex">\(U\)</span> is orthogonal matrix and <span class="arithmatex">\(D\)</span> is diagonal matrix. </p>
<p>Basically <span class="arithmatex">\(X\)</span> is variable that we can choose.</p>
<p>If <span class="arithmatex">\(d=1\)</span>, <span class="arithmatex">\(|Ux| = |x|\)</span>, denote <span class="arithmatex">\(u = Ux\)</span>the objective function becomes <span class="arithmatex">\(u^{T}Du\)</span>. </p>
<p>Assume the elements of <span class="arithmatex">\(D\)</span> are sorted as decreasing order. <span class="arithmatex">\(u = e_{1}\)</span>.</p>
<p>So <span class="arithmatex">\(x = U^{T}e_{1}\)</span> or x is the first column of <span class="arithmatex">\(U\)</span>.</p>
<h3 id="implementation-the-power-iteration">Implementation (The Power Iteration)</h3>
<p>The key problems of PCA are singular values and singular vectors.</p>
<p>Singular polynomial is hard to find roots ...</p>
<p>Maybe there are some ways to find reductions.</p>
<p>But here the motivation is that we pick a vector <span class="arithmatex">\(x\)</span> and apply to operator many times.</p>
<p>Theorem, for any <span class="arithmatex">\(\delta, \epsilon &gt; 0\)</span>, letting <span class="arithmatex">\(v_{1}\)</span> denote the top eigenvector of <span class="arithmatex">\(A\)</span>, with probability at least <span class="arithmatex">\(1-\delta\)</span> over the choice of <span class="arithmatex">\(u_{0}\)</span>,</p>
<div class="arithmatex">\[|\langle \frac{A^{t}u_{0}}{|A^{t}u_{0}|}, v_{1} \rangle| \geq 1 - \epsilon\]</div>
<p>provided </p>
<div class="arithmatex">\[t &gt; O(\frac{\log d + \log \frac{1}{\epsilon} + \log \frac{1}{\delta}}{\log \frac{\lambda_{1}}{\lambda_{2}}})\]</div>
<p>where <span class="arithmatex">\(\frac{\lambda_{1}}{\lambda_{2}}\)</span> is the spectral gap.</p>
<p>proof </p>
<p>let <span class="arithmatex">\(v_{1}, \cdots, v_{k}\)</span> be k orthonormal vectors.</p>
<div class="arithmatex">\[
\begin{aligned}
|\langle \frac{A^{t}u_{0}}{|A^{t}u_{0}|}, v_{1} \rangle | 
&amp;= |\frac{\langle u_{0}, v_{1} \rangle \lambda_{1}^{t}}{\sqrt{\sum_{i=1}^{d}\langle u_{0}, v_{i} \rangle^{2} \lambda_{i}^{2t}}}| \\
&amp;\geq |\frac{\langle u_{0}, v_{1}\rangle \lambda^{t}}{\sqrt{\langle u_{0}, v_{1} \rangle^2 \lambda_{1}^{2t} + \lambda_{2}^{2t}}}| \\
&amp;\geq |\frac{\langle u_{0}, v_{i} \rangle \lambda^{t}}{|\langle u_{0}, v_{1} \rangle| \lambda_{1}^{t} + \lambda_{2}^{t}}|\\
&amp;= |\frac{1}{1 + \frac{1}{\langle u_{0}, v_{1} \rangle } \frac{\lambda_{2}}{\lambda_{1}}^{t}}|
\end{aligned}
\]</div>
<p>So, let this &lt; <span class="arithmatex">\(\epsilon\)</span>.</p>
<div class="arithmatex">\[ t &gt; \frac{\log |\frac{1}{\langle u_{0}, v_{1} \rangle}| + \log \frac{1}{\epsilon}}{\log |\frac{\lambda_{1}}{\lambda_{2}}|}\]</div>
<p>Someone told me that <span class="arithmatex">\(\langle u_{0}, v_{1} \rangle &gt; \frac{\delta}{2\sqrt{d}}\)</span> with probability <span class="arithmatex">\(1 - \delta\)</span>. (I do not how to prove this.) </p>
<p>so <span class="arithmatex">\(\log \frac{1}{\langle u_{0}, v_{1} \rangle}  &lt; \log d + \log \frac{1}{\delta}\)</span> which completes the proof.</p>
<h2 id="low-rank-matrix-decomposition">Low-rank Matrix Decomposition</h2>
<h3 id="svd">SVD</h3>
<p>The key method is the SVD(Singular Value Decomposition) which states that every matrix <span class="arithmatex">\(A\)</span> can be interpreted as <span class="arithmatex">\(USV^{T}\)</span></p>
<p>I do not willing to include the proof here, because that the constructive proof shows few motivation.</p>
<p>Some motivations are here </p>
<div class="arithmatex">\[A^{T}A = (USV^{T})^{T} USV^{T} = VS^{T}SV^{T}\]</div>
<p>According to PCA, V contains eigen-vectors of <span class="arithmatex">\(A\)</span>.</p>
<p>Also </p>
<div class="arithmatex">\[ AA^{T} = (VS^{T}U^{T})^{T}(VS^{T}U^{T}) = USS^{T}U^{T} \]</div>
<p>The similarity holds for <span class="arithmatex">\(U\)</span>.</p>
<p>Actually SVD links the eigenvalues of <span class="arithmatex">\(A\)</span> and <span class="arithmatex">\(A^{T}\)</span></p>
<p>Also these facts help us to calculate the SVD(just use power iteration).</p>
<h3 id="low-rank-matrix">Low-rank Matrix</h3>
<p>Recall the matrix with rank <span class="arithmatex">\(k\)</span> can be interpret as <span class="arithmatex">\(\sum_{i=1}^{k}\textbf{u}_{i} \textbf{v}_{i}^{T}\)</span>.</p>
<p>under Frobenius norm, it can be shown the SVD derives the best approximation.</p>
<p>Frobenius norm </p>
<div class="arithmatex">\[||M||_{F} = \sqrt{\sum_{i,j}m^{2}_{i,j}}\]</div>
<p>for any matrix <span class="arithmatex">\(A\)</span> and its rank-k approximation using SVD <span class="arithmatex">\(A_{k}\)</span>.</p>
<p>Then for any rank-k matrix <span class="arithmatex">\(B\)</span>.</p>
<p>Then </p>
<div class="arithmatex">\[||A - A_{k}||_{F} \leq ||A - B||_{F}\]</div>
<hr />
<p>Although we can approximate the matrix under Frobenius norm, but the decomposition is not unique.</p>
<div class="arithmatex">\[A_{k} = U_{k}V^{T}_{k} = U_{k}B^{-1}BT^{T}_{k} = (U_{k}B^{-1})(T_{k}B^{T})^{T}\]</div>
<p>But if we extend matrix to tensor, something will happen.</p>
<h2 id="low-rank-tensor-decomposition">Low-rank tensor Decomposition</h2>
<p>A rank-k <span class="arithmatex">\(n \times n \times n\)</span> tensor <span class="arithmatex">\(A\)</span> can be interpret as </p>
<div class="arithmatex">\[A_{x,y,z} = \sum_{i=1}^{k} u_{i}(x)v_{i}(y)w_{i}(z) \]</div>
<p>ATTENTION: <span class="arithmatex">\((u_{1}, \cdots, u_{k})\)</span> linearly independent, same for <span class="arithmatex">\(v's\)</span> and <span class="arithmatex">\(w's\)</span></p>
<p>And we can use the notation <span class="arithmatex">\(\oplus\)</span>.</p>
<div class="arithmatex">\[A = \sum_{i=1}^{k} u_{i} \oplus v_{i} \oplus w_{i}\]</div>
<p>Theorem: Given a 3-tensor with rank <span class="arithmatex">\(k\)</span>, the decomposition is unique(up to scale a constant)</p>
<h3 id="jenrichs-algorithm">Jenrich's algorithm</h3>
<ol>
<li>
<p>random two unit vectors <span class="arithmatex">\(x, y \in \mathbb{R}^{n}\)</span> </p>
</li>
<li>
<p>define <span class="arithmatex">\(A_{x}, A_{y}\)</span>.</p>
</li>
</ol>
<div class="arithmatex">\[A_{x}(a, b) = \sum_{i=1}^{k} u_{i}(a) v_{i}(b)\sum_{j=1}^{n}w_{i}(j)x(j) = \sum_{i=1}^{k} u_{i}(a) v_{i}(b) \langle w, x_{i} \rangle\]</div>
<div class="arithmatex">\[A_{y}(a, b) = \sum_{i=1}^{k} u_{i}(a) v_{i}(b)\sum_{j=1}^{n}w_{i}(j)y(j) = \sum_{i=1}^{k} u_{i}(a) v_{i}(b) \langle w, y_{i} \rangle\]</div>
<ol>
<li>compute </li>
</ol>
<div class="arithmatex">\[A_{x}A_{y}^{-1} = QSQ^{-1}\]</div>
<div class="arithmatex">\[A_{x}^{-1}A_{y} = (Y^{T})^{-1}TY^{T}\]</div>
<ol>
<li>
<p>with probability 1, <span class="arithmatex">\(Q\)</span> describes <span class="arithmatex">\((u_{1}, u_{2}, \cdots, u_{k})\)</span> and <span class="arithmatex">\(Y\)</span> describes <span class="arithmatex">\((v_{1}, v_{2}, \cdots, v_{k})\)</span>.</p>
</li>
<li>
<p>solve the linear system to compute <span class="arithmatex">\((w_{1}, w_{2}, \cdots, w_{k})\)</span>.</p>
</li>
</ol>
<p>correctness: </p>
<div class="arithmatex">\[A_{x} = UDV^{T}, A_{y} = UEV^{T}\]</div>
<div class="arithmatex">\[A_{x}A^{-1}_{y} = UDE^{-1}U^{-1}\]</div>
<div class="arithmatex">\[A_{x}^{-1}A_{y} = V^{T^{-1}}D^{-1}EV^{T}\]</div>
<p>if <span class="arithmatex">\(x, y\)</span> pick randomly, it's likely that <span class="arithmatex">\(A_{x}A_{y}^{-1}\)</span> and <span class="arithmatex">\(A^{-1}_{x}A_{y}\)</span>'s eigenvalues are distinct, so we can distinguish different eigenvectors.</p>
<p>And because of the <span class="arithmatex">\(S, T\)</span> should be the reciprocative, so we can group correct eigenvectors.</p>
<h2 id="spectral-graph-theory">Spectral Graph Theory</h2>
<p>The magic about representing the graph as matrix.</p>
<p>Denote rank matrix as <span class="arithmatex">\(D\)</span> and adjacent matrix as <span class="arithmatex">\(A\)</span>.</p>
<p>Define Laplacian matrix as <span class="arithmatex">\(L = D - A\)</span>.</p>
<p>Note <span class="arithmatex">\(L\)</span> is a symmetric matrix.</p>
<p>now consider</p>
<div class="arithmatex">\[
Lv_{i} = deg(i)v_{i} - \sum_{j\sim i} v_{j} = \sum_{j \sim i} (v_{i} - v_{j})
\]</div>
<div class="arithmatex">\[
\begin{align}
v^{T}Lv &amp;= \sum_{i=1}^{n} v_{i} \sum_{j \sim i} (v_{i} - v_{j}) \\
&amp;= \sum_{i&lt;j: j \sim i}(v_{i} - v_{j})^{2}
\end{align}
\]</div>
<p>So if we put all the vertices on the real numberline. 
<span class="arithmatex">\(v^{T}Lv\)</span> is the square sum of all the path.</p>
<h3 id="eigenvalues-and-eigenvectors">Eigenvalues and Eigenvectors</h3>
<p><span class="arithmatex">\(L\)</span> has <span class="arithmatex">\(n\)</span> non-negative real eigenvalues because <span class="arithmatex">\(L\)</span> is a symmetric matrix and <span class="arithmatex">\(v^{T}Lv \geq 0\)</span>. </p>
<p>The fact is that the minimum eigenvalue is <span class="arithmatex">\(0\)</span>. Let <span class="arithmatex">\(v = (\frac{1}{\sqrt{n}}, \cdots, \frac{1}{\sqrt{n}})\)</span>.</p>
<div class="arithmatex">\[Lv_{i} = \sum_{j \sim i} (v_{i} - v_{j}) = 0\]</div>
<p>Theorem: The number of zero eigenvalues of the Laplacian matrix equals the number connected components of the graph.</p>
<p>proof</p>
<p>we first show that # connected components &lt; # zero eigenvalues</p>
<p>Let <span class="arithmatex">\(S_{i}\)</span> be a maximal connected components, construct a vector <span class="arithmatex">\(v\)</span>, <span class="arithmatex">\(v_{i} = \frac{1}{\sqrt{|S_{i}|}}  \mathbb{I}[x \in S_{i}]\)</span></p>
<p>So it can form # connected component orthonormal vectors.</p>
<p>Then about the other side, recall <span class="arithmatex">\(v^{T}Lv\)</span>, if <span class="arithmatex">\(v_{k+1}\)</span> is orthogonal to <span class="arithmatex">\(v_{1}, \cdots, v_{k}\)</span>, assume <span class="arithmatex">\(v_{k+1, j} \neq 0\)</span>, then for positions of the same maximal connected component must <span class="arithmatex">\(\neq 0\)</span>, so it must not orthogonal to all vectors.</p>
<p><span class="arithmatex">\(\blacksquare\)</span></p>
<h3 id="conductance-and-isoperimeter">Conductance and isoperimeter</h3>
<p>Some intuitions are that <span class="arithmatex">\(v^{T}Lv = \lambda v^{T}v = \sum_{i \sim j} (v_{i} - v_{j})^{2}\)</span>, the 2nd smallest eigenvalue somehow place vertices ``near''.</p>
<p>This fact can be used to embed vertices into a <span class="arithmatex">\(d\)</span>-dimensional space. Map vertex <span class="arithmatex">\(i\)</span> to <span class="arithmatex">\((v_{2}(i), v_{3}(i))\)</span> making the adjacancy near. While choose the 2 largest eigenvector will bring them apart.</p>
<p>This also lead to the implementation in clustering and graph coloring.</p>
<p>Definition: isoperimeter of a cut <span class="arithmatex">\(\theta(S)\)</span></p>
<div class="arithmatex">\[\theta(S) = \frac{|\delta(S)|}{\min(|S|, |V - S|)}\]</div>
<p>The isoperimeter ratio of the graph <span class="arithmatex">\(G\)</span> is </p>
<div class="arithmatex">\[\min_{S \subset V(G)} \delta(S)\]</div>
<p>This number interpret the connectivity of a graph.</p>
<p>And the second smallest eigenvalue <span class="arithmatex">\(\lambda_{2}\)</span> (maybe <span class="arithmatex">\(\lambda_{2} = 0\)</span>) will give some insight of this number like those intuitions above.</p>
<p>Theorem: Given a graph <span class="arithmatex">\(G = (V, E)\)</span> and any set <span class="arithmatex">\(S \subset V\)</span>, it holds that</p>
<div class="arithmatex">\[\delta(S) \geq \lambda_{2} (1 - \frac{\min(|S|, |V - S|)}{|V|})\]</div>
<p>proof:</p>
<p>We assume that <span class="arithmatex">\(|S| &lt; |V - S|\)</span>.</p>
<p>We can interpret <span class="arithmatex">\(\delta(S)\)</span> by <span class="arithmatex">\(v^{T}Lv\)</span>.</p>
<p>Define <span class="arithmatex">\(v\)</span> as when <span class="arithmatex">\(i \notin S\)</span>, <span class="arithmatex">\(v(i) = -\frac{|S|}{|V|}\)</span>, while <span class="arithmatex">\(i \in S\)</span>, <span class="arithmatex">\(v(i) = 1 - \frac{|S|}{|V|}\)</span>.</p>
<div class="arithmatex">\[v^{T}Lv = \sum_{(i,j) \in \delta(S)} (v_{i} - v_{j})^{2} = |\delta(S)|\]</div>
<p>Recall that if <span class="arithmatex">\(\lambda_{1}, \cdots, \lambda_{n}\)</span> and <span class="arithmatex">\(v_{1}, \cdots, v_{n}\)</span> are eigenvalues and eigenvectors of <span class="arithmatex">\(L\)</span>.</p>
<div class="arithmatex">\[v^{T}Lv = \sum_{i=1}^{n} \langle v, v_{i} \rangle v_{i}^{T}v_{i}\]</div>
<p>So if we extract the eigenvalue <span class="arithmatex">\(0\)</span> and corresponding eigenvector <span class="arithmatex">\((\frac{1}{\sqrt{n}}, \cdots, \frac{1}{\sqrt{n}})\)</span>. </p>
<p>So</p>
<div class="arithmatex">\[\lambda_{2} = \min_{x: x^{T}v_{1} = 0} \frac{v^{T}Lv}{v^{T}v} \leq \frac{|\delta(S)|}{|S|(1 - \frac{|S|}{|v|})}\]</div>
<p>So</p>
<div class="arithmatex">\[|\delta(S)| \geq \lambda_{2} |S|(1 - \frac{|S|}{|V|})\]</div>
<p><span class="arithmatex">\(\blacksquare\)</span></p>
<p>Cheeger's Theorem</p>
<p>If <span class="arithmatex">\(\lambda_{2}\)</span> is the second smallest eigenvalue of a d-regular graph, Then there exists a set <span class="arithmatex">\(S \subset V\)</span> such that</p>
<div class="arithmatex">\[\frac{\lambda_{2}}{2d} \leq \mathrm{cond}(S) \leq \frac{\sqrt{2\lambda_{2}}}{\sqrt{d}}\]</div>
<h3 id="diffusion-model-and-random-walks">Diffusion model and Random walks</h3>
<p>Let <span class="arithmatex">\(w: V \rightarrow \mathbb{R}\)</span>, define this process <span class="arithmatex">\(w_{i+1}(x) = \frac{1}{\mathrm{Adj}(x)}\sum_{x\sim y}w_{i}(y)\)</span></p>
<p>So we know that <span class="arithmatex">\(w_{i+1} = D^{-1}Aw_{i}\)</span></p>
<p>This is actually the power iteration. </p>
<h2 id="sampling-and-estimation">Sampling and estimation</h2>
<h3 id="reservoir-sampling">Reservoir Sampling</h3>
<p>want to sample <span class="arithmatex">\(k\)</span> data from totally <span class="arithmatex">\(N\)</span> data uniformly. </p>
<p>Just recall how to implement permutation in C++.</p>
<p>uniform random swap this datum with some element existing.</p>
<h3 id="markov-inequality-and-chebyshevs-inequality">Markov inequality and Chebyshev's inequality</h3>
<p>all included by the ''counting and sampling''</p>
<h3 id="importance-sampling">Importance Sampling</h3>
<p>intuitions are sample more important data (long-tail distribution) or much variance data (cut down variance).</p>
<h3 id="estimating-the-missing-mass">Estimating the missing mass</h3>
<p>Good-Turing frequency estimation scheme.</p>
<div class="arithmatex">\[\mathbb{Pr}[next draw is something new] \approx \frac{\# elements\quad seen\quad once}{n}\]</div>
<h3 id="mcmc">MCMC</h3>
<p>Theorem Fundamental theory of Markov Chain</p>
<ol>
<li>
<p>for every two states <span class="arithmatex">\(s_{i}, s_{j}\)</span>, it is possible to eventually get to state <span class="arithmatex">\(s_{j}\)</span>, if one starts in state <span class="arithmatex">\(s_{i}\)</span>.</p>
</li>
<li>
<p>The chain is aperiodic: for a pair states, <span class="arithmatex">\(s_{i}\)</span>, <span class="arithmatex">\(s_{j}\)</span>, consider the set of time <span class="arithmatex">\(\{t_{1}, t_{2},\cdots \}\)</span> consisting of all <span class="arithmatex">\(t\)</span> for which <span class="arithmatex">\(\mathbb{Pr}[X_{t}=s_{j}|X_{0}=s_{i}]&gt;0\)</span>. A chain is aperiodic if $gcd({t_{1}, t_{2},\cdots })=1  $</p>
</li>
</ol>
<p>Then for any states <span class="arithmatex">\(s\)</span>,</p>
<div class="arithmatex">\[\lim_{t \rightarrow \infty}D(t,s) \rightarrow \pi\]</div>
<p>other information will be included by Counting and sampling.</p>
<h2 id="discrete-fourier-transform-and-convolution">Discrete Fourier Transform and Convolution</h2>
<p>As a mathematical tool, Fourier transform has a variety of implementations.</p>
<p>There are also different versions to define Fourier transform.</p>
<h3 id="matrix-form">Matrix form</h3>
<div class="arithmatex">\[\mathcal{F}(v) = M_{n}v\]</div>
<p>Let <span class="arithmatex">\(w_{n} = e^{-\frac{2\pi i}{n}}\)</span>. Then <span class="arithmatex">\(M_{i,j} = w_{n}^{ij}\)</span>. </p>
<p>NB <span class="arithmatex">\(M_{n}\)</span> is 0-indexed.</p>
<p>To get the inverse of <span class="arithmatex">\(M\)</span>, first consider <span class="arithmatex">\(M^{2}\)</span></p>
<p>consider <span class="arithmatex">\(k\)</span> th column of <span class="arithmatex">\(M\)</span> is <span class="arithmatex">\((e^{-\frac{2\pi i 0}{n}}, e^{-\frac{2\pi ik}{n}}, e^{-\frac{2\pi i2k}{n}}, \cdots, e^{-\frac{2\pi i(n-1)k}{n}})^{T}\)</span></p>
<p><span class="arithmatex">\(M\)</span> is a symmetric matrix. So its <span class="arithmatex">\(p\)</span> th row is <span class="arithmatex">\((e^{-\frac{2\pi i0}{n}}, e^{-\frac{2\pi ip}{n}}, e^{-\frac{2\pi i 2p}{n}}, \cdots, e^{-\frac{2\pi i(n-1)p}{n}})\)</span></p>
<p>compute inner-product</p>
<p>When <span class="arithmatex">\(1 \neq e^{- \frac{2\pi i (k+p)}{n}}\)</span></p>
<div class="arithmatex">\[\sum_{j=0}^{n-1} e^{-\frac{2\pi ij(k+p)}{n}} = \frac{1 - e^{-2\pi i(k+q)}}{1 - e^{-\frac{2\pi i(k+q)}{n}}}\]</div>
<p>So <span class="arithmatex">\(M^{2}\)</span> is analogous with the <span class="arithmatex">\(\mathbb{I}_{n}\)</span></p>
<p>Let <span class="arithmatex">\(k\)</span> th column of <span class="arithmatex">\(M\)</span> be <span class="arithmatex">\(M^{i}\)</span>. Then </p>
<div class="arithmatex">\[M^{-1} = \frac{1}{n}(M^{1}, M^{n}, M^{n-1}, \cdots, M^{2})\]</div>
<p>We can also use inverse representation of <span class="arithmatex">\(w_{n}\)</span>.</p>
<h3 id="polynomial-evaluation-and-interpolation">Polynomial evaluation and interpolation</h3>
<p>Matrix in that form can be view as linear polynomial evaluation on specific points set.</p>
<p>Both the evaluation and interpolation can be done in <span class="arithmatex">\(O(n\log n)\)</span> time.</p>
<p>Also the killer method is the convolution. </p>
<h3 id="change-of-basis">Change of basis</h3>
<p>The discrete Fourier transform can be viewed as a way of basis decomposition like the continuous version of Fourier transform.</p>
<h3 id="the-fast-fourier-transform">The Fast Fourier transform</h3>
<p>Discrete fourier transform can be done in <span class="arithmatex">\(O(n\log n)\)</span> time much faster than the matrix multiplication.</p>
<p>Intuitively <span class="arithmatex">\(M\)</span> are well-structured. </p>
<h3 id="convolution">Convolution</h3>
<p>There are many ways to define convolution. One is using polynomial multiplication.</p>
<p>Definition: the convolution of two vectors <span class="arithmatex">\(v, w\)</span> of respective length <span class="arithmatex">\(n, m\)</span> is denoted <span class="arithmatex">\(v*u = u*v\)</span> and is defined to be the vector of coefficients of the product of the polynomials associated to <span class="arithmatex">\(v\)</span> and <span class="arithmatex">\(w\)</span>, <span class="arithmatex">\(P_{v}(x)P_{w}(x)\)</span>.</p>
<p>Fact: convolution can be interpreted by Fourier transform as</p>
<div class="arithmatex">\[v*w = \mathcal{F}^{-1}(\mathcal{F}(v) \times \mathcal{F}(w))\]</div>
<p>NB here we do not specify the dimension of Fourier transform. But with the view of polynomial multiplication, the dimension is at least <span class="arithmatex">\(n + m\)</span>.</p>
<p>(The continuous version of Fourier transform usually has infinite dimensions.)</p>
<p>Also if <span class="arithmatex">\(q = v*w\)</span>, then</p>
<div class="arithmatex">\[v = \mathcal{F}^{-1}(\mathcal{F}(q)./\mathcal{F}(w))\]</div>
<p>Another definition: Any transformation of a vector that is linear and translational invariant is a convolution.</p>
<p>Can be used in differential equations and physical force.(?)</p></div>
            </div>
        </div>

        <footer class="col-md-12">
            <hr>
            <p>Documentation built with <a href="https://www.mkdocs.org/">MkDocs</a>.</p>
        </footer>
        <script>
            var base_url = "../../..",
                shortcuts = {"help": 191, "next": 78, "previous": 80, "search": 83};
        </script>
        <script src="../../../js/base.js" defer></script>
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6" defer></script>
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" defer></script>
        <script src="../../../search/main.js" defer></script>

        <div class="modal" id="mkdocs_search_modal" tabindex="-1" role="dialog" aria-labelledby="searchModalLabel" aria-hidden="true">
    <div class="modal-dialog modal-lg">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="searchModalLabel">Search</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
                <p>From here you can search these documents. Enter your search terms below.</p>
                <form>
                    <div class="form-group">
                        <input type="search" class="form-control" placeholder="Search..." id="mkdocs-search-query" title="Type search term here">
                    </div>
                </form>
                <div id="mkdocs-search-results" data-no-results-text="No results found"></div>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div><div class="modal" id="mkdocs_keyboard_modal" tabindex="-1" role="dialog" aria-labelledby="keyboardModalLabel" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="keyboardModalLabel">Keyboard Shortcuts</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
              <table class="table">
                <thead>
                  <tr>
                    <th style="width: 20%;">Keys</th>
                    <th>Action</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td class="help shortcut"><kbd>?</kbd></td>
                    <td>Open this help</td>
                  </tr>
                  <tr>
                    <td class="next shortcut"><kbd>n</kbd></td>
                    <td>Next page</td>
                  </tr>
                  <tr>
                    <td class="prev shortcut"><kbd>p</kbd></td>
                    <td>Previous page</td>
                  </tr>
                  <tr>
                    <td class="search shortcut"><kbd>s</kbd></td>
                    <td>Search</td>
                  </tr>
                </tbody>
              </table>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div>

    </body>
</html>
